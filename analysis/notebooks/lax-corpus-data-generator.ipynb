{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAX Corpus 1k dataframe generator\n",
    "\n",
    "Grabs data from cogtoolslab server and creates CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../../../stimuli\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "# import drawing_utils as drawing\n",
    "# import importlib\n",
    "# import scoring\n",
    "\n",
    "sys.path.append(\"../../stimuli/towers/block_utils/\")\n",
    "import blockworld_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "results_csv_directory = \"../../results/csv\"\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv(os.path.join(analysis_dir,'../../auth.txt'), header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "\n",
    "db = conn['lax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomains = {\n",
    "    'structures' :  ['bridge', 'castle', 'house', 'city'],\n",
    "    'drawing' :  ['nuts-bolts','wheels','furniture','dials']\n",
    "}\n",
    "\n",
    "domains = list(subdomains.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_names = ['corpus_prolific_test', 'corpus_prolific_test_3'] # 2 intentionally left out\n",
    "experiment_template = \"lax-{}-{}-corpus-{}-10\"\n",
    "condition = 'procedural'\n",
    "expected_trials = 10\n",
    "\n",
    "df_trial = pd.DataFrame()\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for domain in domains:\n",
    "    col_name = 'lax_{}_corpus'.format(domain)\n",
    "    coll = db[col_name]\n",
    "    \n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        # get all data for subdomain from db\n",
    "        df_subdomain_all = pd.DataFrame(coll.find({\"$and\":[  {'iterationName' : { '$in': iteration_names }},\n",
    "                                          {'experimentName': experiment_template.format(domain, subdomain, condition)},\n",
    "                                         ]}))\n",
    "        \n",
    "        if len(df_subdomain_all) > 0:\n",
    "\n",
    "            df_subdomain_all['domain'] = domain\n",
    "            df_subdomain_all['subdomain'] = subdomain\n",
    "\n",
    "\n",
    "            # get metadata\n",
    "            df_subdomain_meta = df_subdomain_all[(df_subdomain_all.datatype == 'stim_metadata')]\\\n",
    "                                        [[\"gameID\",\"partitionFamily\",\"splitNumber\",\"stimIDs\", \"stimURLS\", \"stimGroups\",\n",
    "                                          \"numGames\",\"experimentType\",\"experimentName\",\"versionInd\"]]\n",
    "\n",
    "            # get trial data\n",
    "            df_subdomain_trial = df_subdomain_all[\\\n",
    "                      (df_subdomain_all.trial_type == 'stimuli-contextual-language-production') &\n",
    "                      (df_subdomain_all.datatype == 'trial_end') &\n",
    "                      (~pd.isna(df_subdomain_all.stimId))]\\\n",
    "                      [['datatype', 'iterationName', 'condition', 'domain', 'subdomain',\n",
    "                        'config_name', 'gameID', 'shuffle', 'trialOrder', 'rt', 'workerID', \n",
    "                        'trial_type', 'trial_index', 'time_elapsed', 'internal_node_id',\n",
    "                        'view_history', 'stimId', 'stimURL', 'responses']]\n",
    "\n",
    "            # merge metadata into trial data\n",
    "\n",
    "            # verify stim groups in metadata are correct\n",
    "            dicts = list(df_subdomain_all[df_subdomain_all.datatype=='stim_metadata']['stimGroups'])\n",
    "            stim_groups = reduce(lambda dict1, dict2: {**dict1, **dict2}, dicts)\n",
    "            stim_groups['demo_stim'] = 'demo_stim'\n",
    "            # assign stim groups from metadata\n",
    "            df_subdomain_trial['stim_group'] = df_subdomain_trial['stimId'].apply(lambda stim: stim_groups[stim])\n",
    "            df_subdomain_trial = df_subdomain_trial.merge(df_subdomain_meta, how='left', on='gameID')\n",
    "\n",
    "            # append subdomain data to main dataframe\n",
    "            df_trial = df_trial.append(df_subdomain_trial, ignore_index=True)\n",
    "            \n",
    "            df_all = df_all.append(df_subdomain_all, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print('no data for ' + domain + '.' + subdomain)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark completed datasets\n",
    "# find full datasets\n",
    "did_complete = df_trial[df_trial.stim_group != 'demo_stim'].groupby(['gameID']).count()['datatype'] == expected_trials\n",
    "complete_dataset_gameIDs = list(did_complete[did_complete].index)\n",
    "\n",
    "df_trial.loc[:,'complete_dataset'] = False\n",
    "df_trial.loc[(df_trial.gameID.isin(complete_dataset_gameIDs)), 'complete_dataset'] = True\n",
    "df_all.loc[:,'complete_dataset'] = False\n",
    "df_all.loc[(df_all.gameID.isin(complete_dataset_gameIDs)), 'complete_dataset'] = True\n",
    "\n",
    "# assign correct trial number\n",
    "df_trial.loc[:,'trial_num'] = df_trial.trial_index - min(df_trial.trial_index.unique()[1:]) + 1\n",
    "# assign practice trials to trial_num = 0\n",
    "df_trial.loc[df_trial.trial_num < 0,'trial_num'] = 0\n",
    "\n",
    "df_trial['rt_mins'] = df_trial.rt/(60*1000)\n",
    "\n",
    "df_trial.loc[:, 'responses'] = df_trial.responses.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find datasets with no trials with 8 steps\n",
    "def get_responses(response):\n",
    "\n",
    "    whats = [key for key in response.keys() if 'what' in key]\n",
    "    wheres = [key for key in response.keys() if 'where' in key]\n",
    "\n",
    "    what_responses = [response[what] for what in whats]\n",
    "    where_responses = [response[where] for where in wheres]\n",
    "\n",
    "    return (what_responses, where_responses)\n",
    "\n",
    "df_trial.loc[:, 'response_lists'] = df_trial.responses.apply(get_responses)\n",
    "df_trial.loc[:, 'whats'] = df_trial.response_lists.apply(lambda x:x[0])\n",
    "df_trial.loc[:, 'wheres'] = df_trial.response_lists.apply(lambda x:x[1])\n",
    "df_trial.loc[:, 'n_steps'] = df_trial.whats.apply(len)\n",
    "\n",
    "df_trial.loc[:, 'what_messages_lengths'] = df_trial.whats.apply(lambda responses: [len(response) for response in responses])\n",
    "df_trial.loc[:, 'where_messages_lengths'] = df_trial.wheres.apply(lambda responses: [len(response) for response in responses])\n",
    "\n",
    "df_trial.loc[:, 'what_char_sum'] = df_trial.what_messages_lengths.apply(np.sum)\n",
    "df_trial.loc[:, 'where_char_sum'] = df_trial.where_messages_lengths.apply(np.sum)\n",
    "\n",
    "df_trial.loc[:, 'char_sum'] = df_trial.what_char_sum + df_trial.where_char_sum \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark those that hit 8 step limit\n",
    "hit_8_step_limit = df_trial.groupby('gameID').n_steps.unique().apply(max) == 8\n",
    "\n",
    "df_trial.loc[:, 'ppt_hit_8_step_limit'] = (df_trial.iterationName == 'corpus_prolific_test') & \\\n",
    "                                        (df_trial.gameID.apply(lambda id: hit_8_step_limit[id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if correct multiple of trials\n",
    "assert(df_trial[df_trial.stim_group != 'demo_stim'].groupby('complete_dataset').count()\\\n",
    "           ['trial_num'][True] % expected_trials == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many complete datasets?\n",
    "df_trial[(df_trial.complete_dataset) & (df_trial.trial_num > 0)].groupby(['domain','subdomain'])['rt'].count()/expected_trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic linguistic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['processed_whats'] = [list(nlp.pipe(text)) for text in df_trial['whats']]\n",
    "df_trial['lemmatized_whats'] = [[[str(w.lemma_.lower()) for w in sentence] for sentence in text] for text in df_trial['processed_whats']]\n",
    "df_trial['whats_pos'] = [[[str(w.pos_) for w in sentence] for sentence in text] for text in df_trial['processed_whats']]\n",
    "df_trial['lemmatized_notstop_whats'] = [[[str(w.lemma_.lower()) for w in sentence if (not w.is_stop)] for sentence in text] for text in df_trial['processed_whats']]\n",
    "df_trial['lemmatized_filtered_whats'] = [[[str(w.lemma_.lower()) for w in sentence if not(w.pos_ in ['DET','PUNCT'])] for sentence in text] for text in df_trial['processed_whats']]\n",
    "df_trial['n_whats_filtered'] = df_trial['lemmatized_filtered_whats'].apply(lambda x: sum([len(sub) for sub in x]))\n",
    "\n",
    "df_trial['processed_wheres'] = [list(nlp.pipe(text)) for text in df_trial['wheres']]\n",
    "df_trial['lemmatized_wheres'] = [[[str(w.lemma_.lower()) for w in sentence] for sentence in text] for text in df_trial['processed_wheres']]\n",
    "df_trial['wheres_pos'] = [[[str(w.pos_) for w in sentence] for sentence in text] for text in df_trial['processed_wheres']]\n",
    "df_trial['lemmatized_notstop_wheres'] = [[[str(w.lemma_.lower()) for w in sentence if (not w.is_stop)] for sentence in text] for text in df_trial['processed_wheres']]\n",
    "df_trial['lemmatized_filtered_wheres'] = [[[str(w.lemma_.lower()) for w in sentence if not(w.pos_ in ['DET','PUNCT'])] for sentence in text] for text in df_trial['processed_wheres']]\n",
    "df_trial['n_wheres_filtered'] = df_trial['lemmatized_filtered_wheres'].apply(lambda x: sum([len(sub) for sub in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.loc[:,'unique_whats'] = \\\n",
    "    df_trial.lemmatized_filtered_whats.apply(lambda xss: pd.unique([x for xs in xss for x in xs]))\n",
    "df_trial.loc[:,'unique_wheres'] = \\\n",
    "    df_trial.lemmatized_filtered_whats.apply(lambda xss: pd.unique([x for xs in xss for x in xs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.loc[:,'n_unique_whats'] = df_trial.unique_whats.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.loc[:, 'what_word_sum'] = df_trial['lemmatized_filtered_whats'].apply(lambda x: sum([len(sub) for sub in x]))\n",
    "df_trial.loc[:, 'where_word_sum'] = df_trial['lemmatized_filtered_whats'].apply(lambda x: sum([len(sub) for sub in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check dataset is complete (i.e. >= 2 annotations for each structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_trial[(df_trial.stimId!='demo_stim') &\n",
    "         (df_trial.complete_dataset) & \n",
    "         (~df_trial.ppt_hit_8_step_limit)]\\\n",
    "    .groupby(['subdomain','stimId'])['responses'].count() >= 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop prolific ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial = df_trial.drop('workerID', axis=1)\n",
    "df_all = df_all.drop('workerID', axis=1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole corpus\n",
    "\n",
    "really_save = True\n",
    "\n",
    "if really_save:\n",
    "    df_trial.to_csv(results_csv_directory + '/lax_corpus_1k_trial_unfiltered.csv')\n",
    "    df_all.to_csv(results_csv_directory + '/lax_corpus_1k_all_unfiltered.csv')\n",
    "    \n",
    "# print(results_csv_directory + '/lax_corpus_' + iteration_name + '_' + condition + '_trial.csv')\n",
    "# print(results_csv_directory + '/lax_corpus_' + iteration_name + '_' + condition + '_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove demo stimuli and participants who hit the 8-step limit before it was removed\n",
    "df_trial = (df_trial\n",
    "            .query('stimId != \"demo_stim\"')\n",
    "            .query('~ppt_hit_8_step_limit')\n",
    "            .query('complete_dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add flags for unusual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT less than 5 seconds\n",
    "df_trial.loc[:, 'short_rt'] = df_trial.rt < 5000\n",
    "\n",
    "# RT greater than 10 mins\n",
    "df_trial.loc[:, 'long_rt'] = df_trial.rt > 600000\n",
    "\n",
    "# Gave same response for more than one stimulus\n",
    "df_trial.loc[:,'responses_str'] = df_trial['responses'].apply(str)\n",
    "duplicate_responders = \\\n",
    "    list(df_trial.groupby(['gameID']).filter(lambda x: max(x['responses_str'].value_counts()) > 1).gameID.unique())\n",
    "df_trial.loc[:, 'duplicate_responder'] = df_trial.gameID.isin(duplicate_responders)\n",
    "\n",
    "# Unusually short descriptions (<2 words per cell)\n",
    "df_trial.loc[:, 'length_outlier'] = df_trial.what_word_sum < 3\n",
    "\n",
    "# Referring to pay/ money\n",
    "off_task_words = ['paid', 'money', 'pay']\n",
    "df_trial.loc[:,'off_task_flag'] = df_trial.unique_whats.apply(\\\n",
    "                                    lambda whats: len(set(off_task_words).intersection(set(whats))) > 0) | \\\n",
    "                                  df_trial.unique_wheres.apply(\\\n",
    "                                    lambda wheres: len(set(off_task_words).intersection(set(wheres))) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join trial data with program data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcoded DSLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structures_topdown = pd.DataFrame()\n",
    "\n",
    "for subdomain in ['bridge','castle','city', 'house']:\n",
    "    df_subdomain = pd.read_csv(\"https://github.com/CatherineWong/drawingtasks/raw/main/data/summaries/{}_programs_all.csv\".format(subdomain))\n",
    "    df_structures_topdown = df_structures_topdown.append(df_subdomain, ignore_index=True)\n",
    "\n",
    "df_structures_topdown.loc[:,'subdomain'] = df_structures_topdown.structure_type\n",
    "df_structures_topdown.loc[:,'domain'] = 'structures'\n",
    "df_structures_topdown.loc[:,'stimId'] =  df_structures_topdown.structure_number.apply(lambda x: str(x).zfill(3))\n",
    "df_structures_topdown = df_structures_topdown.drop(columns=['Unnamed: 0','Unnamed: 0.1','structure_type','structure_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drawing_topdown = pd.DataFrame()\n",
    "\n",
    "for subdomain in ['dials','furniture','nuts_bolts','wheels']:\n",
    "\n",
    "    df_subdomain = pd.read_csv(\"https://github.com/CatherineWong/drawingtasks/raw/main/data/summaries/{}_programs_all.csv\".format(subdomain))\n",
    "    df_drawing_topdown = df_drawing_topdown.append(df_subdomain, ignore_index=True)\n",
    "\n",
    "df_drawing_topdown.loc[:,'subdomain'] = df_drawing_topdown.task_name.apply(lambda x: x.split('_')[0])\n",
    "df_drawing_topdown.loc[:,'domain'] = 'drawing'\n",
    "df_drawing_topdown.loc[:,'stimId'] = df_drawing_topdown.s3_stimuli.apply(lambda x: x.split('-')[-1].split('.')[0])\n",
    "df_drawing_topdown = df_drawing_topdown.drop(columns=['task_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drawing_topdown.loc[df_drawing_topdown.subdomain == 'nuts','subdomain'] = 'nuts-bolts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topdown = df_drawing_topdown.append(df_structures_topdown, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = \\\n",
    "    df_trial.merge(df_topdown, how='left', on=['stimId','subdomain','domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries = {}\n",
    "\n",
    "for level in ['low','mid','high']: #,'tower'\n",
    "    \n",
    "    libraries[level] = []\n",
    " \n",
    "    df_structures_topdown.loc[:, level+'_level_parts'] = df_structures_topdown.loc[:, level+'_level_parts'].apply(ast.literal_eval)\n",
    "    df_structures_topdown[level+'_level_prog_length'] = df_structures_topdown[level+'_level_parts'].apply(len)\n",
    "    df_structures_topdown[level+'_level_prog_unique_tokens'] = df_structures_topdown[level+'_level_parts'].apply(lambda x: len(np.unique(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still waiting on handcoded DSLs from Cathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole corpus\n",
    "\n",
    "really_save = True\n",
    "\n",
    "if really_save:\n",
    "    df_topdown.to_csv(results_csv_directory + '/lax_corpus_1k_programs_cogsci22.csv')\n",
    "    df_combined.to_csv(results_csv_directory + '/lax_corpus_1k_trials_cogsci22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drawing_topdown['domain'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check exit survey responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list exit survey comments\n",
    "\n",
    "list(df_all[(df_all.trial_type=='survey-text') & (df_all.iterationName=='corpus_prolific_test_3')]\\\n",
    "         ['response'].apply(lambda x: x['Q0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load base program data (Not used for cogsci22, but links to s3 stimulus dataframes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gadgets\n",
    "# df_drawing_programs = pd.DataFrame()\n",
    "\n",
    "# for drawing_subdomain in subdomains['drawing']:\n",
    "#     summary_domain = 'nuts_bolts' if drawing_subdomain == 'nuts-bolts' else drawing_subdomain\n",
    "    \n",
    "#     df_sub = pd.read_csv('./gadget_programs_tmp/{}_all.csv'.format(summary_domain))\n",
    "#     df_sub.loc[:,'domain'] = 'drawing'\n",
    "#     df_sub.loc[:,'subdomain'] = drawing_subdomain\n",
    "#     df_drawing_programs = df_drawing_programs.append(df_sub, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# # df_drawing_programs.dreamcoder_program_dsl_0_tokens = df_drawing_programs.dreamcoder_program_dsl_0_tokens.apply(ast.literal_eval)\n",
    "# df_drawing_programs.loc[:,'stimId'] = df_drawing_programs.s3_stimuli.apply(lambda x: x.split('-')[-1].split('.')[0])\n",
    "\n",
    "# # will be correct later when we have all dsls. Currently this only works for nuts-bolts and dials\n",
    "# # df_drawing_programs.loc[:,'n_tokens'] = df_drawing_programs.dreamcoder_program_dsl_0_tokens.apply(lambda x: len(ast.literal_eval(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # structures\n",
    "# df_structures_programs = pd.DataFrame()\n",
    "\n",
    "# for structure_subdomain in subdomains['structures']:\n",
    "#     df_sub = pd.read_csv('https://lax-structures-{}-all.s3.amazonaws.com/df_{}.csv'.format(structure_subdomain,\n",
    "#                                                                                            structure_subdomain))\n",
    "#     df_structures_programs = df_structures_programs.append(df_sub, ignore_index=True)\n",
    "    \n",
    "# # make columns consistent with trial dataframe\n",
    "# df_structures_programs.loc[:,'stimId'] = df_structures_programs.structure_number.apply(lambda x: str(x).zfill(3))\n",
    "# df_structures_programs.loc[:,'subdomain'] = df_structures_programs.structure_type\n",
    "# df_structures_programs.loc[:,'domain'] = 'structures'\n",
    "\n",
    "# df_structures_programs = df_structures_programs.rename(columns={'dreamcoder_program':'dreamcoder_program_dsl_auto_generated'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge drawing program data into df_trial\n",
    "# df_combined = df_trial.merge(df_drawing_programs, how='left', on=['stimId','subdomain','domain'])\n",
    "\n",
    "# # then merge structure program data into result \n",
    "# df_combined = df_combined.merge(df_structures_programs, how='left', on=['stimId','subdomain','domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
