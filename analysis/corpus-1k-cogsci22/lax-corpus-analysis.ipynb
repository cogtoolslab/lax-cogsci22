{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e53265",
   "metadata": {},
   "source": [
    "# Language abstraction analysis notebook\n",
    "\n",
    "## Corpus\n",
    "\n",
    "This notebook collects analyses performed for the LAX cogsci 2022 paper, as well as some additional analyses.\n",
    "\n",
    "Statistics were largely done outside of this notebook, in R (see lax_corpus_statistics.Rmd and mixed-effects-model.Rmd in this folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01025ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomains = {\n",
    "    'structures' :  ['bridge','city','house','castle'],\n",
    "    'drawing' :  ['nuts-bolts','wheels','dials','furniture']\n",
    "}\n",
    "\n",
    "domains = list(subdomains.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04fe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../../../stimuli\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.spatial.distance as distance\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont, ImageColor\n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import random\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from IPython.display import clear_output, Image, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# sys.path.append(\"../../stimuli/towers/block_utils/\")\n",
    "# import blockworld_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aedf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# styling for paper_figures\n",
    "\n",
    "sns.set_style('white', {'axes.linewidth': 0.5})\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "\n",
    "LIGHT_BLUE = \"#56B0CD\"\n",
    "LIGHT_ORANGE = \"#FFCE78\"\n",
    "LIGHT_GREEN = \"#95C793\"\n",
    "LIGHT_RED = \"#CC867A\"\n",
    "\n",
    "BLUE = \"#009BCD\"\n",
    "ORANGE = \"#FFA300\"\n",
    "GREEN = \"#688B67\"\n",
    "RED = \"#CC5945\"\n",
    "\n",
    "DARK_BLUE   = \"#0E4478\"\n",
    "DARK_ORANGE = \"#A46400\"\n",
    "DARK_GREEN  = \"#275C4A\"\n",
    "DARK_RED    =  \"#9B3024\"\n",
    "\n",
    "domain_palettes_light = {\n",
    "    \n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: LIGHT_BLUE,\n",
    "        subdomains[domains[0]][1]: LIGHT_ORANGE,\n",
    "        subdomains[domains[0]][2]: LIGHT_GREEN, \n",
    "        subdomains[domains[0]][3]: LIGHT_RED   \n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: LIGHT_BLUE,\n",
    "        subdomains[domains[1]][1]: LIGHT_ORANGE,\n",
    "        subdomains[domains[1]][2]: LIGHT_GREEN, \n",
    "        subdomains[domains[1]][3]: LIGHT_RED \n",
    "    }\n",
    "}\n",
    "\n",
    "domain_palettes = {\n",
    "    \n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: BLUE,\n",
    "        subdomains[domains[0]][1]: ORANGE,\n",
    "        subdomains[domains[0]][2]: GREEN,\n",
    "        subdomains[domains[0]][3]: RED\n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: BLUE,\n",
    "        subdomains[domains[1]][1]: ORANGE,\n",
    "        subdomains[domains[1]][2]: GREEN,\n",
    "        subdomains[domains[1]][3]: RED\n",
    "    }\n",
    "}\n",
    "\n",
    "domain_palettes_dark = {\n",
    "    \n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: DARK_BLUE,\n",
    "        subdomains[domains[0]][1]: DARK_ORANGE,\n",
    "        subdomains[domains[0]][2]: DARK_GREEN, \n",
    "        subdomains[domains[0]][3]: DARK_RED   \n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: DARK_BLUE,\n",
    "        subdomains[domains[1]][1]: DARK_ORANGE,\n",
    "        subdomains[domains[1]][2]: DARK_GREEN, \n",
    "        subdomains[domains[1]][3]: DARK_RED \n",
    "    }\n",
    "}\n",
    "\n",
    "N=256\n",
    "gradients = []\n",
    "\n",
    "for light, mid, dark in zip([LIGHT_BLUE,LIGHT_ORANGE,LIGHT_GREEN,LIGHT_RED],[BLUE,ORANGE,GREEN,RED],[DARK_BLUE,DARK_ORANGE,DARK_GREEN,DARK_RED]):\n",
    "    light_rgb = list(ImageColor.getcolor(light, \"RGB\"))\n",
    "    mid_rgb = list(ImageColor.getcolor(mid, \"RGB\"))\n",
    "    dark_rgb = list(ImageColor.getcolor(dark, \"RGB\"))\n",
    "    vals = np.ones((N, 4))\n",
    "    vals[:, 0] = np.append(np.linspace(light_rgb[0]/255, mid_rgb[0]/255, int(N/2)),np.linspace(mid_rgb[0]/255, dark_rgb[0]/255, int(N/2))) # R\n",
    "    vals[:, 1] = np.append(np.linspace(light_rgb[1]/255, mid_rgb[1]/255, int(N/2)),np.linspace(mid_rgb[1]/255, dark_rgb[1]/255, int(N/2))) # G\n",
    "    vals[:, 2] = np.append(np.linspace(light_rgb[2]/255, mid_rgb[2]/255, int(N/2)),np.linspace(mid_rgb[2]/255, dark_rgb[2]/255, int(N/2))) # B\n",
    "    newcmp = ListedColormap(vals)\n",
    "    \n",
    "    gradients.append(newcmp)\n",
    "\n",
    "domain_gradients = {\n",
    "\n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: gradients[0],\n",
    "        subdomains[domains[0]][1]: gradients[1],\n",
    "        subdomains[domains[0]][2]: gradients[2],\n",
    "        subdomains[domains[0]][3]: gradients[3],\n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: gradients[0],\n",
    "        subdomains[domains[1]][1]: gradients[1],\n",
    "        subdomains[domains[1]][2]: gradients[2],\n",
    "        subdomains[domains[1]][3]: gradients[3],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numbers_and_space(responses):\n",
    "    responses = [f\"{id}: {response}\" for (id, response) in enumerate(responses)]\n",
    "    responses = '\\n'.join(responses)\n",
    "    return responses\n",
    "\n",
    "def group_by_stim_url(df, config_name):\n",
    "    df[config_name] = df[['stimURL','responses']].groupby(['stimURL'])['responses'].transform(lambda responses: add_numbers_and_space(responses))\n",
    "    df[['stimURL', config_name]].drop_duplicates()\n",
    "    return df[['stimURL', config_name]]\n",
    "\n",
    "def group_by_stim_id(df, config_name):\n",
    "    df[config_name] = df[['stimId','responses']].groupby(['stimId'])['responses'].transform(lambda responses: add_numbers_and_space(responses))\n",
    "    df[['stimId', config_name]].drop_duplicates()\n",
    "    return df[['stimId', config_name]]\n",
    "\n",
    "def path_to_image_html(path):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "\n",
    "    return '<img src=\"'+ path + '\" style=max-width:100px \" />'\n",
    "\n",
    "\n",
    "def stimId_to_s3URL(domain, subdomain, stimID):\n",
    "    \n",
    "    if domain == 'structures':\n",
    "        url =  \"https://lax-{}-{}-all.s3.amazonaws.com/\".format(domain, \n",
    "                                                                subdomain)\\\n",
    "               + \"lax-{}-{}-{}-all.png\".format(domain,\n",
    "                                      subdomain,\n",
    "                                      str(stimID).zfill(3))\n",
    "    else: #check this\n",
    "        url =  \"https://lax-{}-{}-all.s3.amazonaws.com/\".format(domain, \n",
    "                                                                subdomain)\\\n",
    "               + \"lax-{}-{}-all-{}.png\".format(domain,\n",
    "                                      subdomain,\n",
    "                                      str(stimID).zfill(3))\n",
    "\n",
    "    return url\n",
    "    \n",
    "\n",
    "def stimId_to_html(stimId, domain = 'structures', subdomain = 'bridge'):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "    stimURL = stimId_to_s3URL(domain, subdomain, stimId) \n",
    "    return '<img src=\"'+ stimURL + '\" style=max-width:150px \" />'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "\n",
    "results_csv_directory = \"../../results/csv/\"\n",
    "# df_trial = pd.read_csv(os.path.join(results_csv_directory, 'lax_corpus_1k_trial.csv'))\n",
    "df_trial = pd.read_csv(os.path.join(results_csv_directory, 'lax_corpus_1k_trials_cogsci22.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d062093",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b08e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### column name descriptions\n",
    "\n",
    "```\n",
    "id\n",
    "'datatype': \n",
    "'iterationName':\n",
    "'config_name':\n",
    "    \n",
    "'condition':\n",
    "'domain': structures/ drawing\n",
    "'subdomain': \n",
    "'gameID': uuid for participant\n",
    "\n",
    "'shuffle':\n",
    "'trialOrder':\n",
    "\n",
    "'rt': reaction time\n",
    "'rt_mins': reaction time in minutes\n",
    "\n",
    "'trial_index': jspsych trial number (not experimental)\n",
    "'trial_type':\n",
    "'time_elapsed': \n",
    "'complete_dataset': did participant submit 10 responses?\n",
    "'trial_num': trial number\n",
    "    \n",
    "'responses': complete response of what and where messages\n",
    "'response_lists': same as above, but list of lists\n",
    "'whats': list of what responses\n",
    "'wheres': list of where responses\n",
    "'n_steps': number of steps\n",
    "'what_messages_lengths': list of lengths of what responses (characters)\n",
    "'where_messages_lengths': list of lengths of where responses (characters)\n",
    "'what_char_sum': total characters in what responses\n",
    "'where_char_sum': total characters in where responses\n",
    "'char_sum': total characters in responses \n",
    "'ppt_hit_8_step_limit': participant was in version of experiment with 8 steps, and hit this limit on at least one trial\n",
    "\n",
    "\n",
    "'lemmatized_whats': lemmatized by spacy\n",
    "'lemmatized_wheres':\n",
    "'lemmatized_notstop_whats': lemmatized by spacy, stop words (incl numbers) removed\n",
    "'lemmatized_notstop_wheres': \n",
    "'lemmatized_filtered_whats': lemmatized by spacy, determiners, punctuation and symbols removed\n",
    "'lemmatized_filtered_wheres':\n",
    "\n",
    "``` \n",
    "\n",
    "### Metadata    \n",
    "```\n",
    "'internal_node_id':\n",
    "'view_history':\n",
    "'stimId':\n",
    "'stimURL':\n",
    "'stim_group':\n",
    "'partitionFamily':\n",
    "'splitNumber':\n",
    "'stimIDs':\n",
    "'stimURLS':\n",
    "'stimGroups':\n",
    "'numGames':\n",
    "'experimentType':\n",
    "'experimentName':\n",
    "'versionInd':\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253206e5",
   "metadata": {},
   "source": [
    "### Common preprocessing\n",
    "\n",
    "Most preprocessing is dealt with in ./lax_corpus_data_generator.ipynb\n",
    "\n",
    "Here we add preprocessing steps common to several but not all analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf04b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret more complex data structures i.e. lists\n",
    "for column_name in ['responses',\n",
    "                    'whats',\n",
    "                    'wheres',\n",
    "                    'lemmatized_whats',\n",
    "                    'lemmatized_notstop_whats',\n",
    "                    'lemmatized_filtered_whats',\n",
    "                    'lemmatized_wheres',\n",
    "                    'lemmatized_notstop_wheres',\n",
    "                    'lemmatized_filtered_wheres',\n",
    "                    'low_level_parts',\n",
    "                    'mid_level_parts',\n",
    "                    'high_level_parts',\n",
    "                    'low_level_part_types',\n",
    "                    'mid_level_part_types',\n",
    "                    'high_level_part_types',\n",
    "                    'low_level_part_params',\n",
    "                    'mid_level_part_params',\n",
    "                    'high_level_part_params',\n",
    "                    'dreamcoder_program_dsl_0_tokens'\n",
    "                   ]:\n",
    "    df_trial[column_name] = df_trial[column_name].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_trial.dreamcoder_program_dsl_0_tokens.apply(lambda x: type(x) == list).all()\n",
    "\n",
    "df_trial.loc[:,'base_program_length'] = df_trial.dreamcoder_program_dsl_0_tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cea6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mean word count for each stim\n",
    "what_word_sum_means = df_trial.groupby(['domain','subdomain','stimId']).mean()['what_word_sum'].reset_index()\n",
    "what_word_sum_means = what_word_sum_means.rename(columns={'what_word_sum':'what_word_mean'})\n",
    "\n",
    "#add means to df_trial (only do this if you will take one row per item from df_trial)\n",
    "df_trial = df_trial.merge(what_word_sum_means, how='left', on=['domain','subdomain','stimId']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec779b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_pos = {}\n",
    "for _, row in df_trial.iterrows():\n",
    "    whats_list = row[\"lemmatized_whats\"]\n",
    "    pos_list = ast.literal_eval(row[\"whats_pos\"])\n",
    "    for i in range(len(whats_list)):\n",
    "        item = whats_list[i]\n",
    "        for j in range(len(item)):\n",
    "            word = whats_list[i][j]\n",
    "            pos = pos_list[i][j]\n",
    "            word_to_pos[word] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace common misspellings (applied to top-word analyses below)\n",
    "spelling_map = {w: w for w in word_to_pos.keys()}\n",
    "spelling_map[\"boarder\"] = \"border\"\n",
    "spelling_map[\"centre\"] = \"center\"\n",
    "spelling_map[\"cirlce\"] = \"circle\"\n",
    "spelling_map[\"cirlcle\"] = \"circle\"\n",
    "spelling_map[\"colour\"] = \"color\"\n",
    "spelling_map[\"collumn\"] = \"column\"\n",
    "spelling_map[\"columb\"] = \"column\"\n",
    "spelling_map[\"colum\"] = \"column\"\n",
    "spelling_map[\"hexgon\"] = \"hexagon\"\n",
    "spelling_map[\"heaxgon\"] = \"hexagon\"\n",
    "spelling_map[\"heaxagon\"] = \"hexagon\"\n",
    "spelling_map[\"hexagin\"] = \"hexagon\"\n",
    "spelling_map[\"hexogram\"] = \"hexagon\"\n",
    "spelling_map[\"horiz\"] = \"horizontal\"\n",
    "spelling_map[\"octogon\"] = \"octagon\"\n",
    "spelling_map[\"octogan\"] = \"octagon\"\n",
    "spelling_map[\"rec\"] = \"rectangle\"\n",
    "spelling_map[\"rect\"] = \"rectangle\"\n",
    "spelling_map[\"rectagle\"] = \"rectangle\"\n",
    "spelling_map[\"recagle\"] = \"rectangle\"\n",
    "spelling_map[\"sqaure\"] = \"square\"\n",
    "spelling_map[\"squae\"] = \"square\"\n",
    "spelling_map[\"squar\"] = \"square\"\n",
    "spelling_map[\"sqar\"] = \"square\"\n",
    "spelling_map[\"sqare\"] = \"square\"\n",
    "spelling_map[\"squre\"] = \"square\"\n",
    "spelling_map[\"verticle\"] = \"vertical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340689c1-a9db-4fa8-85e4-ea23ca003493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.to_csv(os.path.join(results_csv_directory, 'lax_corpus_1k_trials_cogsci22_preprocessed.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47362ea",
   "metadata": {},
   "source": [
    "## Analysis of programs\n",
    "\n",
    "- gallery: longest / average-length / shortest programs in each domain\n",
    "- length: \n",
    "- diversity domain-specificity\n",
    "- program token-level distinctiveness\n",
    "- in: domain, subdomain, library_0 vs. library_compressive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ec56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use separate df with one entry per item\n",
    "df_programs =  pd.read_csv('../../results/csv/lax_corpus_1k_programs_cogsci22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a911b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_programs['base_program_length'] = df_programs.dreamcoder_program_dsl_0_tokens.apply\\\n",
    "            (lambda x: len(ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find order of complexity for subdomains\n",
    "df_programs.groupby(['domain','subdomain'])['base_program_length'].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0ced9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with shortest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['structures']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=True)\n",
    "        \n",
    "#         grouped_df_list = [group_by_stim_id(df, config_name) for (config_name, df) in {subdomain: df_subdomain}.items()]\n",
    "#         reduced_df = reduce(lambda x, y: pd.merge(x, y, on = ['stimId','domain','subdomain']), grouped_df_list).drop_duplicates()\n",
    "\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884e9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with longest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['structures']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=False)\n",
    "        \n",
    "#         grouped_df_list = [group_by_stim_id(df, config_name) for (config_name, df) in {subdomain: df_subdomain}.items()]\n",
    "#         reduced_df = reduce(lambda x, y: pd.merge(x, y, on = ['stimId','domain','subdomain']), grouped_df_list).drop_duplicates()\n",
    "\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82ee3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with shortest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['drawing']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=True)\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7564b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with longest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['drawing']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=False)\n",
    "        \n",
    "#         grouped_df_list = [group_by_stim_id(df, config_name) for (config_name, df) in {subdomain: df_subdomain}.items()]\n",
    "#         reduced_df = reduce(lambda x, y: pd.merge(x, y, on = ['stimId','domain','subdomain']), grouped_df_list).drop_duplicates()\n",
    "\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92de21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis of language\n",
    " \n",
    "- Visualize: longest / average-length / shortest word counts in each domain (see: lax-corpus-results-visualizer.ipynb)\n",
    "- token-level diversity: across domains, across subdomains within domain, across stims within subdomain, across participants\n",
    "- token-level distinctiveness (PMI, tf-idf): across domains, across subdomains within domain, across stims within subdomain, across participants\n",
    "- same as above, but now on \"semantic\" representations: gLoVe embeddings / BERT / & co. from huggingface / Spacy has all of these i think-> show a tsne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc753d4",
   "metadata": {},
   "source": [
    "### Characterizing language use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d67fc",
   "metadata": {},
   "source": [
    "#### Number of steps in instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fed6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"n_steps\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=1,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('number of steps in instructions')\n",
    "# plt.savefig('./plots/instruction_steps_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_steps = df_trial[df_trial.domain=='drawing']['n_steps']\n",
    "s_steps = df_trial[df_trial.domain=='structures']['n_steps']\n",
    "\n",
    "stats.ttest_ind(d_steps,s_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num',\n",
    "             y='n_steps', \n",
    "             hue='domain')\n",
    "plt.ylim((1,11))\n",
    "plt.title('total characters across trials, by domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4612a",
   "metadata": {},
   "source": [
    "#### Character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"what_char_sum\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=20,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('total characters in instructions')\n",
    "# plt.savefig('./plots/instruction_chars_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35016d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_chars = df_trial[df_trial.domain=='drawing']['char_sum']\n",
    "s_chars = df_trial[df_trial.domain=='structures']['char_sum']\n",
    "\n",
    "stats.ttest_ind(d_chars,s_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='char_sum', \n",
    "             hue='domain')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('total characters across trials, by domain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359dde0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='char_sum', \n",
    "             hue='subdomain')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('total characters across trials, by subdomain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5176e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='what_char_sum', \n",
    "             hue='domain')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('total WHAT characters across trials, by domain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fe014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='what_char_sum', \n",
    "             hue='subdomain')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('total WHAT characters across trials, by subdomain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num',\n",
    "             y='what_char_sum', \n",
    "             hue='domain', \n",
    "             linestyle='--')\n",
    "\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num', y='where_char_sum', \n",
    "             hue='domain', \n",
    "             linestyle='-', \n",
    "             legend=False)\n",
    "plt.title('total characters across trials, WHAT vs. WHERE, by domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558decd8",
   "metadata": {},
   "source": [
    "### Word-based measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f411b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "\n",
    "plt.figure(figsize=(4,6))\n",
    "sns.barplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='domain', \n",
    "             y='n_whats_filtered')\n",
    "plt.ylabel('unique what words')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response')\n",
    "# plt.savefig('./plots/unique_whats_domain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"n_whats_filtered\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=1,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('number of steps in instructions')\n",
    "# plt.savefig('./plots/instruction_steps_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_whats_filtered = df_trial[df_trial.domain=='drawing']['n_whats_filtered']\n",
    "s_whats_filtered = df_trial[df_trial.domain=='structures']['n_whats_filtered']\n",
    "\n",
    "stats.ttest_ind(d_whats_filtered,s_whats_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_whats_filtered.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_whats_filtered.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bfa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "\n",
    "plt.figure(figsize=(4,6))\n",
    "sns.barplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='domain', \n",
    "             y='n_unique_whats')\n",
    "plt.ylabel('unique what words')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response')\n",
    "# plt.savefig('./plots/unique_whats_domain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69020f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"n_unique_whats\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=1,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('number of steps in instructions')\n",
    "# plt.savefig('./plots/instruction_steps_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ecad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_unique_whats = df_trial[df_trial.domain=='drawing']['n_unique_whats']\n",
    "s_unique_whats = df_trial[df_trial.domain=='structures']['n_unique_whats']\n",
    "\n",
    "stats.ttest_ind(d_unique_whats,s_unique_whats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='subdomain', \n",
    "             y='n_unique_whats',\n",
    "             hue='domain')\n",
    "plt.ylabel('unique what words')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response')\n",
    "# plt.savefig('./plots/unique_whats_subdomain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f185e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='n_unique_whats', \n",
    "             hue='domain')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response, over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd625cc",
   "metadata": {},
   "source": [
    "#### Comparisons between subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19208116",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,8))\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "            data = df_trial,\n",
    "            x = 'subdomain',\n",
    "            order = subdomains['drawing'] +  subdomains['structures'],\n",
    "            palette= {**domain_palettes['drawing'],**domain_palettes_light['structures']},\n",
    "            y = 'what_word_sum')\n",
    "_ = plt.xticks(rotation = 60)\n",
    "# plt.savefig('./plots/what_word_sum_subdomains.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.violinplot(\n",
    "    data = df_trial,\n",
    "    x = 'subdomain',\n",
    "    order = subdomains['drawing'] +  subdomains['structures'],\n",
    "    y = 'what_word_sum',\n",
    "    palette= {**domain_palettes['drawing'],**domain_palettes_light['structures']},\n",
    "    linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.violinplot(\n",
    "    data = df_trial.groupby(['domain','subdomain','stimId']).first().reset_index(),\n",
    "    x = 'subdomain',\n",
    "    order = subdomains['drawing'] +  subdomains['structures'],\n",
    "    palette = {**domain_palettes['drawing'],**domain_palettes_light['structures']},\n",
    "    y = 'what_word_mean',\n",
    "    linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f26d1",
   "metadata": {},
   "source": [
    "### Word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a916280",
   "metadata": {},
   "source": [
    "token-based length: across domains, across subdomains within domain, across stims within subdomain, across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_trial, x=\"what_word_sum\", log_scale=True, fill=False, element=\"step\")\n",
    "sns.histplot(data = df_trial, x=\"where_word_sum\", log_scale=True, fill=False, element=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493182b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_trial,\n",
    "             x=\"what_word_sum\", \n",
    "             hue='domain',\n",
    "             stat=\"density\",\n",
    "             common_norm=False,\n",
    "             log_scale=True,\n",
    "             fill=False,\n",
    "             element=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac167db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean word count (across participants) for each subdomain\n",
    "plt.figure(figsize=(4,6))\n",
    "\n",
    "sns.barplot(data=what_word_sum_means,\n",
    "            x='domain',\n",
    "            y='what_word_mean',\n",
    "            hue_order=['structures','drawing']\n",
    "           )\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('what words')\n",
    "plt.title('word count by domain')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.subplots_adjust(left=0.2, bottom=0.35)\n",
    "# plt.savefig('./plots/what_word_count_domain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f424745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean word count (across participants) for each subdomain\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "sns.barplot(data=what_word_sum_means,\n",
    "            x='subdomain',\n",
    "            hue='domain',\n",
    "            y='what_word_mean',\n",
    "            hue_order=['drawing','structures']\n",
    "           )\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('what words')\n",
    "plt.title('word count by subdomain')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.subplots_adjust(left=0.2, bottom=0.35)\n",
    "# plt.savefig('./plots/what_word_count_subdomain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial[df_trial.subdomain.isin(['nuts-bolts','dials'])], \n",
    "             x=\"what_word_mean\", \n",
    "             hue=\"subdomain\",\n",
    "#              hue_order=['drawing','structures'],\n",
    "             binwidth=5,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('mean number of words in instructions')\n",
    "# plt.savefig('./plots/instruction_chars_dist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc9059",
   "metadata": {},
   "source": [
    "## Top words in domain (by counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc78efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by domain\n",
    "all_words = {}\n",
    "top_words_domain = {}\n",
    "n_words_in_domain = {}\n",
    "tf = {}\n",
    "df = {}\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for domain in domains:\n",
    "    \n",
    "    doc = [d for sublist in df_trial[(df_trial.domain==domain) &\n",
    "                                     (df_trial.complete_dataset)]['lemmatized_filtered_whats'] \n",
    "                                              for item in sublist\n",
    "                                              for d in item]\n",
    "    \n",
    "    doc = [word for word in doc if word_to_pos[word]!='NUM']\n",
    "    c = Counter(doc)\n",
    "    top_words_domain[domain] = c.most_common(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee57224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_words = pd.DataFrame(top_words_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_OFFSET_START = 0\n",
    "Y_OFFSET_START = 1\n",
    "\n",
    "X_OFFSET_INTERVAL = 0.5\n",
    "Y_OFFSET_INTERVAL = 0.1\n",
    "\n",
    "X_OFFSET_WORD = 0.13\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"Greys\")\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "x_offset = X_OFFSET_START\n",
    "for domain in domains:\n",
    "    \n",
    "    y_offset = Y_OFFSET_START\n",
    "\n",
    "#     plt.text(x_offset, y_offset, domain, fontweight=\"bold\", color=cmap(1.0))\n",
    "    y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    domain_counts = np.array([b for (a, b) in top_words_domain[domain]])\n",
    "    \n",
    "    norm = matplotlib.colors.Normalize(vmin=min(-(domain_counts.mean()*3), domain_counts.min()), vmax=domain_counts.max())\n",
    "    \n",
    "    for word, count in top_words_domain[domain]:\n",
    "        alpha = 1\n",
    "        plt.text(x_offset, y_offset, f\"({count:.0f}) \", color=cmap(norm(count)), fontsize=12, alpha=alpha, fontname=\"Arial\")\n",
    "        plt.text(x_offset + X_OFFSET_WORD, y_offset, word, color=cmap(norm(count)), fontsize=16, alpha=alpha)\n",
    "        \n",
    "        y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    x_offset += X_OFFSET_INTERVAL\n",
    "    \n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"top_words.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce99ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Most diagnostic words of subdomain (PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = \"structures\"\n",
    "# DOMAIN = \"drawing\"\n",
    "\n",
    "df_domain = df_trial[(df_trial.domain == DOMAIN) & (df_trial.complete_dataset) & (~df_trial.ppt_hit_8_step_limit) & (df_trial.stimId != 'demo_stim')]\n",
    "df_domain = df_domain.reset_index(drop=True)\n",
    "\n",
    "df_domain[\"lemmatized_whats_flat\"] = df_domain[\"lemmatized_whats\"].map(lambda whats_list: \" \".join([spelling_map[item] for sublist in whats_list for item in sublist if word_to_pos[item] == \"NOUN\"]))\n",
    "\n",
    "\n",
    "df_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents=\"unicode\", min_df=5, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df_domain[\"lemmatized_whats_flat\"])\n",
    "\n",
    "df_counts = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "PSEUDOCOUNT = 1 / len(df_counts.columns)\n",
    "\n",
    "df_counts = df_counts + PSEUDOCOUNT\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pmi = defaultdict(dict)\n",
    "\n",
    "N = df_counts.sum().sum()\n",
    "JOINT_EXP = 1\n",
    "\n",
    "subdomain_priors = ((df_domain.subdomain.value_counts()) / len(df_domain)).to_dict()\n",
    "\n",
    "for subdomain in subdomain_priors:\n",
    "    p_subdomain = np.log2(subdomain_priors[subdomain])\n",
    "    for word in df_counts.columns:\n",
    "        p_joint = np.log2((df_counts[word][df_domain.subdomain == subdomain].sum()) / N)\n",
    "        p_word = np.log2((df_counts[word].sum()) / N)\n",
    "                \n",
    "        pmi = p_joint - (p_word + p_subdomain)\n",
    "        d_pmi[subdomain][word] = pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of positive PMI values\n",
    "df_pmi = pd.DataFrame(d_pmi)\n",
    "(df_pmi > 0).sum().sum() / (df_pmi != 0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({subdomain: df_pmi[subdomain].nlargest(30).index.tolist() for subdomain in subdomain_priors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "df_pmi = df_pmi[subdomains[DOMAIN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb879405",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10\n",
    "\n",
    "X_OFFSET_START = 0\n",
    "Y_OFFSET_START = 1\n",
    "\n",
    "X_OFFSET_INTERVAL = 0.35\n",
    "Y_OFFSET_INTERVAL = 0.1\n",
    "\n",
    "X_OFFSET_WORD = 0.11\n",
    "\n",
    "# CMAPS = map(matplotlib.cm.get_cmap, [\"Blues\", \"Oranges\", \"Greens\", \"Reds\"])\n",
    "CMAPS = gradients\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "x_offset = X_OFFSET_START\n",
    "for subdomain, cmap in zip(df_pmi.columns, CMAPS):\n",
    "    df_pmi_subdomain_top = df_pmi[subdomain].nlargest(TOP_N)\n",
    "    \n",
    "    y_offset = Y_OFFSET_START\n",
    "\n",
    "#     plt.text(x_offset, y_offset, subdomain, fontweight=\"bold\", color=cmap(1.0), fontname=\"Arial\")\n",
    "    y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    norm = matplotlib.colors.Normalize(vmin=min(0, df_pmi_subdomain_top.min()), vmax=df_pmi_subdomain_top.max())\n",
    "    \n",
    "    for word, pmi in df_pmi_subdomain_top.iteritems():\n",
    "        alpha = int(pmi > 0)\n",
    "        plt.text(x_offset, y_offset, f\"({pmi:.2f}) \", color=cmap(norm(pmi)), fontsize=12, alpha=alpha, fontname=\"Arial\")\n",
    "        plt.text(x_offset + X_OFFSET_WORD, y_offset, word, color=cmap(norm(pmi)), fontsize=16, alpha=alpha)\n",
    "        \n",
    "        y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    x_offset += X_OFFSET_INTERVAL\n",
    "    \n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"pmi_{DOMAIN}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd80b4",
   "metadata": {},
   "source": [
    "## Analysis of programs x language\n",
    "\n",
    "- Scatterplot: for each stim AND baseDSL vs. ‘compressive’ DSLL: word count vs. program length; |set(words)| vs.  |set(program_tokens)|\n",
    "  - Questions re: overall trend (sublinear?)\n",
    "  - Questions re: where we see high variation in language length given baseDSL program length \n",
    "  - We can further break these down in: domain, subdomain, annotator, library_0 vs. library_compressive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781219cb",
   "metadata": {},
   "source": [
    "Let's merge the df_structures data frame with trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = sns.scatterplot(x='n_blocks', y = 'what_char_sum', alpha=0.5, data=df_combined)\n",
    "g = sns.jointplot(x='n_blocks', y = 'what_word_sum', alpha = 0.2,  data=df_trial)\n",
    "#g.ax_joint.set_xscale('log')\n",
    "#g.ax_joint.set_yscale('log')\n",
    "#fig.plot([0, 100], [0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef92e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.query('what_word_sum == 1')['responses']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360bcbc",
   "metadata": {},
   "source": [
    "### n stroke vs mean word count for gadget item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9293124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'drawing'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'n_strokes',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "# plt.title(\"number of words by gadget complexity\")\n",
    "plt.xlabel(\"strokes in image\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "s.plot([0,1],[0,1], \n",
    "       transform=s.transAxes, \n",
    "       color='grey',\n",
    "       linestyle='--')\n",
    "\n",
    "# plt.savefig('./plots/gadget_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ee4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df = df_trial[df_trial.domain == 'drawing'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.85, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'n_strokes',\n",
    "        'what_word_mean',\n",
    "        alpha=0.6)\n",
    "\n",
    "\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "    ax.set(xlabel=\"strokes in image\", ylabel=\"mean number of words\")\n",
    "    \n",
    "# plt.savefig('./plots/gadget_language_item_complexity_facet.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c8e53",
   "metadata": {},
   "source": [
    "### n blocks vs mean word count for structure item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9175ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'structures'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'n_blocks',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"blocks in structure\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "s.plot([0,1],[0,1], \n",
    "       transform=s.transAxes, \n",
    "       color='grey',\n",
    "       linestyle='--')\n",
    "\n",
    "# plt.savefig('./plots/structure_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df = df_trial[df_trial.domain == 'structures'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.85, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'n_blocks',\n",
    "        'what_word_mean',\n",
    "        alpha=0.6)\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "    ax.set(xlabel=\"blocks in structure\", ylabel=\"mean number of words\")\n",
    "    \n",
    "# plt.savefig('./plots/structure_language_item_complexity_facet.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18c42c",
   "metadata": {},
   "source": [
    "### base program length vs mean word count for gadget item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'drawing'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'base_program_length',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"base program length\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "# s.plot([0,1],[0,1], \n",
    "#        transform=s.transAxes, \n",
    "#        color='grey',\n",
    "#        linestyle='--')\n",
    "\n",
    "# plt.savefig('./plots/gadget_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a793ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "# remove one outlier to scale graphs\n",
    "df = df_trial[(df_trial.domain == 'drawing') & (df_trial.what_word_mean < 125)].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.8, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                  sharex=False,\n",
    "                  sharey=True,\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'base_program_length',\n",
    "        'what_word_mean',\n",
    "        alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.set_title(None)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xlabel(None)\n",
    "#     ax.set_xlim([0,470])\n",
    "\n",
    "# for ax in s.axes_dict.values():\n",
    "#     ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "#     ax.set(xlabel=\"base_program_length\", ylabel=\"mean number of words\")\n",
    "    \n",
    "# plt.savefig('./plots/gadget_basedsl_langlength_facet.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101599f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdomain in df[\"subdomain\"].unique():\n",
    "    print(subdomain)\n",
    "    df[df[\"subdomain\"] == subdomain]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb96343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"subdomain\"] == subdomain][[\"base_program_length\", \"what_word_mean\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(\n",
    "    x=df[df[\"subdomain\"] == subdomain][\"base_program_length\"],\n",
    "    y=df[df[\"subdomain\"] == subdomain][\"what_word_mean\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e2414",
   "metadata": {},
   "source": [
    "### n blocks vs mean word count for structure item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'structures'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'base_program_length',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"base_program_length\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "# s.plot([0,1],[0,1], \n",
    "#        transform=s.transAxes, \n",
    "#        color='grey',\n",
    "#        linestyle='--')\n",
    "\n",
    "# plt.savefig('./plots/structure_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1408b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df = df_trial[(df_trial.domain == 'structures') & (df_trial.what_word_mean < 150)].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.8, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                  sharex=False,\n",
    "                  sharey=True,\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'base_program_length',\n",
    "        'what_word_mean',\n",
    "        alpha=0.5)\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.set_title(None)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xlabel(None)\n",
    "#     ax.set_xlim([0,470])\n",
    "\n",
    "# for ax in s.axes_dict.values():\n",
    "#     ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "#     ax.set(xlabel=\"base_program_length\", ylabel=\"mean number of words\")\n",
    "    \n",
    "# plt.savefig('./plots/structures_basedsl_langlength_facet.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9c766",
   "metadata": {},
   "source": [
    "## Likelihood ratio test for linear vs. log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.statology.org/likelihood-ratio-test-in-python/\n",
    "\n",
    "likelihood_test_results = []\n",
    "\n",
    "for domain in [\"drawing\", \"structures\"]:\n",
    "    df = df_trial[df_trial.domain == domain].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "    for subdomain in df[\"subdomain\"].unique():\n",
    "        y = df[df[\"subdomain\"] == subdomain][\"what_word_mean\"]\n",
    "\n",
    "        # Reduced model\n",
    "        x = df[df[\"subdomain\"] == subdomain][\"base_program_length\"]\n",
    "        x = sm.add_constant(x)\n",
    "        reduced_model = sm.OLS(y, x).fit()\n",
    "\n",
    "        # Full model\n",
    "        df[\"log_base_program_length\"] = np.log(df[\"base_program_length\"])\n",
    "\n",
    "        x = df[df[\"subdomain\"] == subdomain][[\"base_program_length\", \"log_base_program_length\"]]\n",
    "        x = sm.add_constant(x)\n",
    "        full_model = sm.OLS(y, x).fit()\n",
    "\n",
    "        #calculate likelihood ratio Chi-Squared test statistic\n",
    "        LR_statistic = -2*(reduced_model.llf - full_model.llf)\n",
    "\n",
    "        #calculate p-value of test statistic using 2 degrees of freedom\n",
    "        p_val = scipy.stats.chi2.sf(LR_statistic, 2)\n",
    "\n",
    "        likelihood_test_results.append({\n",
    "            \"domain\": domain,\n",
    "            \"subdomain\": subdomain,\n",
    "            \"chi-squared\": LR_statistic,\n",
    "            \"p_val\": p_val,\n",
    "        })\n",
    "        \n",
    "df_likelihood_test_results = pd.DataFrame(likelihood_test_results)\n",
    "df_likelihood_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331172a4",
   "metadata": {},
   "source": [
    "### Compare distributions of words across domain/ subdomains (JSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ee90f",
   "metadata": {},
   "source": [
    "#### Between domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COUNTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_whats = df_trial.groupby(['gameID','trial_num'])['lemmatized_filtered_whats'].apply(lambda trial_responses: \\\n",
    "    ([x for xs in [word for sublist in trial_responses for word in sublist] for x in xs]))\n",
    "trial_whats_counts = trial_whats.apply(lambda x: Counter(x))\n",
    "\n",
    "df_trial_whats = df_trial[['gameID','trial_num','subdomain', 'domain','lemmatized_filtered_whats']].groupby(['gameID','trial_num']).first()\n",
    "df_trial_whats.loc[:,'trial_whats'] = trial_whats\n",
    "df_trial_whats.loc[:,'what_counts'] = trial_whats_counts\n",
    "\n",
    "all_words = np.unique([x for xs in trial_whats for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceba6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in all_words:\n",
    "    if USE_COUNTS:\n",
    "        df_trial_whats[w] = df_trial_whats['what_counts'].apply(lambda row: int(row[w])) # word counts\n",
    "    else:\n",
    "        df_trial_whats[w] = df_trial_whats['trial_whats'].apply(lambda row: int(w in row)) # present/absent\n",
    "        \n",
    "df_trial_whats = df_trial_whats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate true JSD\n",
    "\n",
    "# word counts for domains\n",
    "drawing_counts = df_trial_whats.loc[(df_trial_whats['domain'] == 'drawing')].iloc[:,10:].sum(axis=0)\n",
    "structures_counts = df_trial_whats.loc[(df_trial_whats['domain'] == 'structures')].iloc[:,10:].sum(axis=0)\n",
    "\n",
    "true_jsd = distance.jensenshannon(drawing_counts,structures_counts,2)\n",
    "print(true_jsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null distribution of JSDs\n",
    "# JSD for distributions of words in domains\n",
    "# Shuffle domain tags. 1000 random assingments to 2 (preserve sizes)\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "n_iters = 1000\n",
    "\n",
    "jsds = []\n",
    "\n",
    "# calculate true split of trials into domains\n",
    "domain_assignments = df_trial.domain.copy()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# for each iteration\n",
    "for n in range(0, n_iters):\n",
    "    \n",
    "    # assign trial random domain tag (following partition of domains in data)\n",
    "    np.random.shuffle(domain_assignments)\n",
    "    \n",
    "    drawing_counts = df_trial_whats.iloc[:,10:][domain_assignments == 'drawing'].sum(axis = 0)\n",
    "    structures_counts = df_trial_whats.iloc[:,10:][domain_assignments == 'structures'].sum(axis = 0)\n",
    "    jsd = distance.jensenshannon(drawing_counts,structures_counts,2)\n",
    "    \n",
    "    # calculate JSD\n",
    "    jsds.append(jsd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a716c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(jsds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report p-value. (how many are greater than true JSD)\n",
    "(sum(jsds > true_jsd) / n_iters) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b03f4",
   "metadata": {},
   "source": [
    "#### Between subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'drawing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate true mean JSD\n",
    "# One domain at a time\n",
    "df_trial_whats_domain = df_trial_whats.loc[(df_trial_whats['domain'] == domain)].reset_index(drop=True).copy()\n",
    "\n",
    "subdomain_counts = {}\n",
    "subdomain_jsds = {}\n",
    "\n",
    "# get counts\n",
    "for subdomain in subdomains[domain]:\n",
    "    subdomain_counts[subdomain] = df_trial_whats_domain.loc[(df_trial_whats['subdomain'] == subdomain)]\\\n",
    "                                    .iloc[:,10:].sum(axis=0)\n",
    "\n",
    "# get JSDS\n",
    "for subdomain_i in subdomains[domain]:\n",
    "    subdomain_jsds[subdomain_i] = {}\n",
    "    for subdomain_j in subdomains[domain]:\n",
    "        subdomain_jsds[subdomain_i][subdomain_j] = distance.jensenshannon(subdomain_counts[subdomain_i],\n",
    "                                                                          subdomain_counts[subdomain_j], 2)\n",
    "        \n",
    "\n",
    "true_subdomain_jsds = pd.DataFrame.from_dict(subdomain_jsds)\n",
    "true_subdomain_jsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true mean JSD\n",
    "true_mean_jsd = np.array(true_subdomain_jsds)[np.triu_indices(4,k = 1)].mean()\n",
    "true_mean_jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db551ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null distribution of mean JSDs\n",
    "\n",
    "df_trial_whats_domain = df_trial_whats.loc[(df_trial_whats['domain'] == domain)].reset_index(drop=True).copy()\n",
    "\n",
    "mean_jsds = []\n",
    "\n",
    "n_iters = 1000\n",
    "\n",
    "# calculate true split of trials into domains\n",
    "subdomain_assignments = df_trial_whats_domain.subdomain.copy()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "for n in range(0,n_iters):\n",
    "    \n",
    "    np.random.shuffle(subdomain_assignments)\n",
    "\n",
    "    subdomain_counts = {}\n",
    "    subdomain_jsds = {}\n",
    "\n",
    "    # get counts\n",
    "    for subdomain in subdomains[domain]:\n",
    "        subdomain_counts[subdomain] = df_trial_whats_domain.iloc[:,10:][subdomain_assignments == subdomain]\\\n",
    "                                        .sum(axis=0)\n",
    "\n",
    "    # get JSDS\n",
    "    for subdomain_i in subdomains[domain]:\n",
    "        subdomain_jsds[subdomain_i] = {}\n",
    "        for subdomain_j in subdomains[domain]:\n",
    "            subdomain_jsds[subdomain_i][subdomain_j] = distance.jensenshannon(subdomain_counts[subdomain_i],\n",
    "                                                                              subdomain_counts[subdomain_j], 2)\n",
    "\n",
    "\n",
    "    subdomain_jsds = pd.DataFrame.from_dict(subdomain_jsds)\n",
    "    mean_jsd = np.array(subdomain_jsds)[np.triu_indices(4,k = 1)].mean()\n",
    "    mean_jsds.append(mean_jsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_jsds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a697650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report p-value. (how many are greater than true JSD)\n",
    "(sum(mean_jsds > true_mean_jsd) / n_iters) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c36c23",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378d11c",
   "metadata": {},
   "source": [
    "### merge urls with top down abstraction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = df_trial[df_trial.domain=='structures'].groupby(['stimId','stimURL','subdomain']).first().reset_index()[['blocks','stimId','stimURL','domain','subdomain']]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topdownabs = pd.read_csv('../../stimuli/towers/df_structures_topdownabs_consistent_abstractions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5decb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topdownabs['stimId'] = df_topdownabs['structure_number']\n",
    "df_topdownabs['subdomain'] = df_topdownabs['structure_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c7ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topdownabs.merge(urls, how='left',on=['blocks','stimId','subdomain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8817b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topdownabs.to_csv('../../stimuli/towers/df_structures_topdownabs_consistent_abstractions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
