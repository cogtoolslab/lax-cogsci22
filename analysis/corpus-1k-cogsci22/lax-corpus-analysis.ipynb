{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e53265",
   "metadata": {},
   "source": [
    "# Language abstraction analysis notebook\n",
    "\n",
    "## Corpus\n",
    "\n",
    "2+ annotations per item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01025ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomains = {\n",
    "    'structures' :  ['bridge','city','house','castle'],\n",
    "    'drawing' :  ['nuts-bolts','wheels','dials','furniture']\n",
    "}\n",
    "\n",
    "domains = list(subdomains.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04fe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../../../stimuli\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.spatial.distance as distance\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont, ImageColor\n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import random\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from IPython.display import clear_output, Image, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# sys.path.append(\"../../stimuli/towers/block_utils/\")\n",
    "# import blockworld_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aedf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# styling for paper_figures\n",
    "\n",
    "sns.set_style('white', {'axes.linewidth': 0.5})\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "\n",
    "LIGHT_BLUE = \"#56B0CD\"\n",
    "LIGHT_ORANGE = \"#FFCE78\"\n",
    "LIGHT_GREEN = \"#95C793\"\n",
    "LIGHT_RED = \"#CC867A\"\n",
    "\n",
    "BLUE = \"#009BCD\"\n",
    "ORANGE = \"#FFA300\"\n",
    "GREEN = \"#688B67\"\n",
    "RED = \"#CC5945\"\n",
    "\n",
    "DARK_BLUE   = \"#0E4478\"\n",
    "DARK_ORANGE = \"#A46400\"\n",
    "DARK_GREEN  = \"#275C4A\"\n",
    "DARK_RED    =  \"#9B3024\"\n",
    "\n",
    "domain_palettes_light = {\n",
    "    \n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: LIGHT_BLUE,\n",
    "        subdomains[domains[0]][1]: LIGHT_ORANGE,\n",
    "        subdomains[domains[0]][2]: LIGHT_GREEN, \n",
    "        subdomains[domains[0]][3]: LIGHT_RED   \n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: LIGHT_BLUE,\n",
    "        subdomains[domains[1]][1]: LIGHT_ORANGE,\n",
    "        subdomains[domains[1]][2]: LIGHT_GREEN, \n",
    "        subdomains[domains[1]][3]: LIGHT_RED \n",
    "    }\n",
    "}\n",
    "\n",
    "domain_palettes = {\n",
    "    \n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: BLUE,\n",
    "        subdomains[domains[0]][1]: ORANGE,\n",
    "        subdomains[domains[0]][2]: GREEN,\n",
    "        subdomains[domains[0]][3]: RED\n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: BLUE,\n",
    "        subdomains[domains[1]][1]: ORANGE,\n",
    "        subdomains[domains[1]][2]: GREEN,\n",
    "        subdomains[domains[1]][3]: RED\n",
    "    }\n",
    "}\n",
    "\n",
    "domain_palettes_dark = {\n",
    "    \n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: DARK_BLUE,\n",
    "        subdomains[domains[0]][1]: DARK_ORANGE,\n",
    "        subdomains[domains[0]][2]: DARK_GREEN, \n",
    "        subdomains[domains[0]][3]: DARK_RED   \n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: DARK_BLUE,\n",
    "        subdomains[domains[1]][1]: DARK_ORANGE,\n",
    "        subdomains[domains[1]][2]: DARK_GREEN, \n",
    "        subdomains[domains[1]][3]: DARK_RED \n",
    "    }\n",
    "}\n",
    "\n",
    "N=256\n",
    "gradients = []\n",
    "\n",
    "for light, mid, dark in zip([LIGHT_BLUE,LIGHT_ORANGE,LIGHT_GREEN,LIGHT_RED],[BLUE,ORANGE,GREEN,RED],[DARK_BLUE,DARK_ORANGE,DARK_GREEN,DARK_RED]):\n",
    "    light_rgb = list(ImageColor.getcolor(light, \"RGB\"))\n",
    "    mid_rgb = list(ImageColor.getcolor(mid, \"RGB\"))\n",
    "    dark_rgb = list(ImageColor.getcolor(dark, \"RGB\"))\n",
    "    vals = np.ones((N, 4))\n",
    "    vals[:, 0] = np.append(np.linspace(light_rgb[0]/255, mid_rgb[0]/255, int(N/2)),np.linspace(mid_rgb[0]/255, dark_rgb[0]/255, int(N/2))) # R\n",
    "    vals[:, 1] = np.append(np.linspace(light_rgb[1]/255, mid_rgb[1]/255, int(N/2)),np.linspace(mid_rgb[1]/255, dark_rgb[1]/255, int(N/2))) # G\n",
    "    vals[:, 2] = np.append(np.linspace(light_rgb[2]/255, mid_rgb[2]/255, int(N/2)),np.linspace(mid_rgb[2]/255, dark_rgb[2]/255, int(N/2))) # B\n",
    "    newcmp = ListedColormap(vals)\n",
    "    \n",
    "    gradients.append(newcmp)\n",
    "\n",
    "domain_gradients = {\n",
    "\n",
    "    domains[0]:{\n",
    "        subdomains[domains[0]][0]: gradients[0],\n",
    "        subdomains[domains[0]][1]: gradients[1],\n",
    "        subdomains[domains[0]][2]: gradients[2],\n",
    "        subdomains[domains[0]][3]: gradients[3],\n",
    "    },\n",
    "     domains[1]:{\n",
    "        subdomains[domains[1]][0]: gradients[0],\n",
    "        subdomains[domains[1]][1]: gradients[1],\n",
    "        subdomains[domains[1]][2]: gradients[2],\n",
    "        subdomains[domains[1]][3]: gradients[3],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numbers_and_space(responses):\n",
    "    responses = [f\"{id}: {response}\" for (id, response) in enumerate(responses)]\n",
    "    responses = '\\n'.join(responses)\n",
    "    return responses\n",
    "\n",
    "def group_by_stim_url(df, config_name):\n",
    "    df[config_name] = df[['stimURL','responses']].groupby(['stimURL'])['responses'].transform(lambda responses: add_numbers_and_space(responses))\n",
    "    df[['stimURL', config_name]].drop_duplicates()\n",
    "    return df[['stimURL', config_name]]\n",
    "\n",
    "def group_by_stim_id(df, config_name):\n",
    "    df[config_name] = df[['stimId','responses']].groupby(['stimId'])['responses'].transform(lambda responses: add_numbers_and_space(responses))\n",
    "    df[['stimId', config_name]].drop_duplicates()\n",
    "    return df[['stimId', config_name]]\n",
    "\n",
    "def path_to_image_html(path):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "\n",
    "    return '<img src=\"'+ path + '\" style=max-width:100px \" />'\n",
    "\n",
    "\n",
    "def stimId_to_s3URL(domain, subdomain, stimID):\n",
    "    \n",
    "    if domain == 'structures':\n",
    "        url =  \"https://lax-{}-{}-all.s3.amazonaws.com/\".format(domain, \n",
    "                                                                subdomain)\\\n",
    "               + \"lax-{}-{}-{}-all.png\".format(domain,\n",
    "                                      subdomain,\n",
    "                                      str(stimID).zfill(3))\n",
    "    else: #check this\n",
    "        url =  \"https://lax-{}-{}-all.s3.amazonaws.com/\".format(domain, \n",
    "                                                                subdomain)\\\n",
    "               + \"lax-{}-{}-all-{}.png\".format(domain,\n",
    "                                      subdomain,\n",
    "                                      str(stimID).zfill(3))\n",
    "\n",
    "    return url\n",
    "    \n",
    "\n",
    "def stimId_to_html(stimId, domain = 'structures', subdomain = 'bridge'):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "    stimURL = stimId_to_s3URL(domain, subdomain, stimId) \n",
    "    return '<img src=\"'+ stimURL + '\" style=max-width:150px \" />'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "\n",
    "results_csv_directory = \"../../results/csv/\"\n",
    "# df_trial = pd.read_csv(os.path.join(results_csv_directory, 'lax_corpus_1k_trial.csv'))\n",
    "df_trial = pd.read_csv(os.path.join(results_csv_directory, 'lax_corpus_1k_trials_cogsci22.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d062093",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b08e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### column name descriptions\n",
    "\n",
    "```\n",
    "id\n",
    "'datatype': \n",
    "'iterationName':\n",
    "'config_name':\n",
    "    \n",
    "'condition':\n",
    "'domain': structures/ drawing\n",
    "'subdomain': \n",
    "'gameID': uuid for participant\n",
    "\n",
    "'shuffle':\n",
    "'trialOrder':\n",
    "\n",
    "'rt': reaction time\n",
    "'rt_mins': reaction time in minutes\n",
    "\n",
    "'trial_index': jspsych trial number (not experimental)\n",
    "'trial_type':\n",
    "'time_elapsed': \n",
    "'complete_dataset': did participant submit 10 responses?\n",
    "'trial_num': trial number\n",
    "    \n",
    "'responses': complete response of what and where messages\n",
    "'response_lists': same as above, but list of lists\n",
    "'whats': list of what responses\n",
    "'wheres': list of where responses\n",
    "'n_steps': number of steps\n",
    "'what_messages_lengths': list of lengths of what responses (characters)\n",
    "'where_messages_lengths': list of lengths of where responses (characters)\n",
    "'what_char_sum': total characters in what responses\n",
    "'where_char_sum': total characters in where responses\n",
    "'char_sum': total characters in responses \n",
    "'ppt_hit_8_step_limit': participant was in version of experiment with 8 steps, and hit this limit on at least one trial\n",
    "\n",
    "\n",
    "'lemmatized_whats': lemmatized by spacy\n",
    "'lemmatized_wheres':\n",
    "'lemmatized_notstop_whats': lemmatized by spacy, stop words (incl numbers) removed\n",
    "'lemmatized_notstop_wheres': \n",
    "'lemmatized_filtered_whats': lemmatized by spacy, determiners, punctuation and symbols removed\n",
    "'lemmatized_filtered_wheres':\n",
    "\n",
    "``` \n",
    "\n",
    "### Metadata    \n",
    "```\n",
    "'internal_node_id':\n",
    "'view_history':\n",
    "'stimId':\n",
    "'stimURL':\n",
    "'stim_group':\n",
    "'partitionFamily':\n",
    "'splitNumber':\n",
    "'stimIDs':\n",
    "'stimURLS':\n",
    "'stimGroups':\n",
    "'numGames':\n",
    "'experimentType':\n",
    "'experimentName':\n",
    "'versionInd':\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253206e5",
   "metadata": {},
   "source": [
    "### Common preprocessing\n",
    "\n",
    "Most preprocessing is dealt with in ./lax_corpus_data_generator.ipynb\n",
    "\n",
    "Here we add preprocessing steps common to several but not all analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf04b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret more complex data structures i.e. lists\n",
    "for column_name in ['responses',\n",
    "                    'whats',\n",
    "                    'wheres',\n",
    "                    'lemmatized_whats',\n",
    "                    'lemmatized_notstop_whats',\n",
    "                    'lemmatized_filtered_whats',\n",
    "                    'lemmatized_wheres',\n",
    "                    'lemmatized_notstop_wheres',\n",
    "                    'lemmatized_filtered_wheres',\n",
    "                    'low_level_parts',\n",
    "                    'mid_level_parts',\n",
    "                    'high_level_parts',\n",
    "                    'low_level_part_types',\n",
    "                    'mid_level_part_types',\n",
    "                    'high_level_part_types',\n",
    "                    'low_level_part_params',\n",
    "                    'mid_level_part_params',\n",
    "                    'high_level_part_params',\n",
    "                    'dreamcoder_program_dsl_0_tokens'\n",
    "                   ]:\n",
    "    df_trial[column_name] = df_trial[column_name].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_trial.dreamcoder_program_dsl_0_tokens.apply(lambda x: type(x) == list).all()\n",
    "\n",
    "df_trial.loc[:,'base_program_length'] = df_trial.dreamcoder_program_dsl_0_tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cea6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mean word count for each stim\n",
    "what_word_sum_means = df_trial.groupby(['domain','subdomain','stimId']).mean()['what_word_sum'].reset_index()\n",
    "what_word_sum_means = what_word_sum_means.rename(columns={'what_word_sum':'what_word_mean'})\n",
    "\n",
    "#add means to df_trial (only do this if you will take one row per item from df_trial)\n",
    "df_trial = df_trial.merge(what_word_sum_means, how='left', on=['domain','subdomain','stimId']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec779b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_pos = {}\n",
    "for _, row in df_trial.iterrows():\n",
    "    whats_list = row[\"lemmatized_whats\"]\n",
    "    pos_list = ast.literal_eval(row[\"whats_pos\"])\n",
    "    for i in range(len(whats_list)):\n",
    "        item = whats_list[i]\n",
    "        for j in range(len(item)):\n",
    "            word = whats_list[i][j]\n",
    "            pos = pos_list[i][j]\n",
    "            word_to_pos[word] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace common misspellings (applied to top-word analyses below)\n",
    "spelling_map = {w: w for w in word_to_pos.keys()}\n",
    "spelling_map[\"boarder\"] = \"border\"\n",
    "spelling_map[\"centre\"] = \"center\"\n",
    "spelling_map[\"cirlce\"] = \"circle\"\n",
    "spelling_map[\"cirlcle\"] = \"circle\"\n",
    "spelling_map[\"colour\"] = \"color\"\n",
    "spelling_map[\"collumn\"] = \"column\"\n",
    "spelling_map[\"columb\"] = \"column\"\n",
    "spelling_map[\"colum\"] = \"column\"\n",
    "spelling_map[\"hexgon\"] = \"hexagon\"\n",
    "spelling_map[\"heaxgon\"] = \"hexagon\"\n",
    "spelling_map[\"heaxagon\"] = \"hexagon\"\n",
    "spelling_map[\"hexagin\"] = \"hexagon\"\n",
    "spelling_map[\"hexogram\"] = \"hexagon\"\n",
    "spelling_map[\"horiz\"] = \"horizontal\"\n",
    "spelling_map[\"octogon\"] = \"octagon\"\n",
    "spelling_map[\"octogan\"] = \"octagon\"\n",
    "spelling_map[\"rec\"] = \"rectangle\"\n",
    "spelling_map[\"rect\"] = \"rectangle\"\n",
    "spelling_map[\"rectagle\"] = \"rectangle\"\n",
    "spelling_map[\"recagle\"] = \"rectangle\"\n",
    "spelling_map[\"sqaure\"] = \"square\"\n",
    "spelling_map[\"squae\"] = \"square\"\n",
    "spelling_map[\"squar\"] = \"square\"\n",
    "spelling_map[\"sqar\"] = \"square\"\n",
    "spelling_map[\"sqare\"] = \"square\"\n",
    "spelling_map[\"squre\"] = \"square\"\n",
    "spelling_map[\"verticle\"] = \"vertical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340689c1-a9db-4fa8-85e4-ea23ca003493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.to_csv(os.path.join(results_csv_directory, 'lax_corpus_1k_trials_cogsci22_preprocessed.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47362ea",
   "metadata": {},
   "source": [
    "## Analysis of programs\n",
    "\n",
    "- gallery: longest / average-length / shortest programs in each domain\n",
    "- length: \n",
    "- diversity domain-specificity\n",
    "- program token-level distinctiveness\n",
    "- in: domain, subdomain, library_0 vs. library_compressive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ec56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use separate df with one entry per item\n",
    "df_programs =  pd.read_csv('../../results/csv/lax_corpus_1k_programs_cogsci22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a911b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_programs['base_program_length'] = df_programs.dreamcoder_program_dsl_0_tokens.apply\\\n",
    "            (lambda x: len(ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find order of complexity for subdomains\n",
    "df_programs.groupby(['domain','subdomain'])['base_program_length'].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0ced9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with shortest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['structures']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=True)\n",
    "        \n",
    "#         grouped_df_list = [group_by_stim_id(df, config_name) for (config_name, df) in {subdomain: df_subdomain}.items()]\n",
    "#         reduced_df = reduce(lambda x, y: pd.merge(x, y, on = ['stimId','domain','subdomain']), grouped_df_list).drop_duplicates()\n",
    "\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884e9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with longest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['structures']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=False)\n",
    "        \n",
    "#         grouped_df_list = [group_by_stim_id(df, config_name) for (config_name, df) in {subdomain: df_subdomain}.items()]\n",
    "#         reduced_df = reduce(lambda x, y: pd.merge(x, y, on = ['stimId','domain','subdomain']), grouped_df_list).drop_duplicates()\n",
    "\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82ee3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with shortest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['drawing']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=True)\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7564b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display items for each structures subdomain with longest program\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for domain in ['drawing']:\n",
    "    for subdomain in subdomains[domain]:\n",
    "        \n",
    "        df_subdomain = df_programs[(df_programs.domain == domain) & (df_programs.subdomain == subdomain)]\\\n",
    "                        [['domain','subdomain','stimId','base_program_length']].sort_values('base_program_length',ascending=False)\n",
    "        \n",
    "#         grouped_df_list = [group_by_stim_id(df, config_name) for (config_name, df) in {subdomain: df_subdomain}.items()]\n",
    "#         reduced_df = reduce(lambda x, y: pd.merge(x, y, on = ['stimId','domain','subdomain']), grouped_df_list).drop_duplicates()\n",
    "\n",
    "        \n",
    "        display(HTML(df_subdomain.head(top_n)\\\n",
    "                        .to_html(escape=False,\n",
    "                                formatters=dict(stimId=\n",
    "                                                lambda x:(stimId_to_html(x, domain = domain, subdomain = subdomain))))\n",
    "                        .replace(\"\\\\n\",\"<br>=======<br><br>\")))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92de21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis of language\n",
    " \n",
    "- Visualize: longest / average-length / shortest word counts in each domain (see: lax-corpus-results-visualizer.ipynb)\n",
    "- token-level diversity: across domains, across subdomains within domain, across stims within subdomain, across participants\n",
    "- token-level distinctiveness (PMI, tf-idf): across domains, across subdomains within domain, across stims within subdomain, across participants\n",
    "- same as above, but now on \"semantic\" representations: gLoVe embeddings / BERT / & co. from huggingface / Spacy has all of these i think-> show a tsne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc753d4",
   "metadata": {},
   "source": [
    "### Characterizing language use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d67fc",
   "metadata": {},
   "source": [
    "#### Number of steps in instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fed6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"n_steps\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=1,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('number of steps in instructions')\n",
    "plt.savefig('./plots/instruction_steps_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_steps = df_trial[df_trial.domain=='drawing']['n_steps']\n",
    "s_steps = df_trial[df_trial.domain=='structures']['n_steps']\n",
    "\n",
    "stats.ttest_ind(d_steps,s_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num',\n",
    "             y='n_steps', \n",
    "             hue='domain')\n",
    "plt.ylim((1,11))\n",
    "plt.title('total characters across trials, by domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4612a",
   "metadata": {},
   "source": [
    "#### Character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"what_char_sum\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=20,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('total characters in instructions')\n",
    "plt.savefig('./plots/instruction_chars_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35016d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_chars = df_trial[df_trial.domain=='drawing']['char_sum']\n",
    "s_chars = df_trial[df_trial.domain=='structures']['char_sum']\n",
    "\n",
    "stats.ttest_ind(d_chars,s_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='char_sum', \n",
    "             hue='domain')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('total characters across trials, by domain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359dde0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='char_sum', \n",
    "             hue='subdomain')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('total characters across trials, by subdomain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5176e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='what_char_sum', \n",
    "             hue='domain')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('total WHAT characters across trials, by domain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fe014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over time\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='what_char_sum', \n",
    "             hue='subdomain')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('total WHAT characters across trials, by subdomain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num',\n",
    "             y='what_char_sum', \n",
    "             hue='domain', \n",
    "             linestyle='--')\n",
    "\n",
    "sns.lineplot(data=df_trial[(df_trial.stimId != 'demo_stim') & \n",
    "                           (df_trial.complete_dataset)], \n",
    "             x='trial_num', y='where_char_sum', \n",
    "             hue='domain', \n",
    "             linestyle='-', \n",
    "             legend=False)\n",
    "plt.title('total characters across trials, WHAT vs. WHERE, by domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558decd8",
   "metadata": {},
   "source": [
    "### Word-based measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f411b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "\n",
    "plt.figure(figsize=(4,6))\n",
    "sns.barplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='domain', \n",
    "             y='n_whats_filtered')\n",
    "plt.ylabel('unique what words')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response')\n",
    "plt.savefig('./plots/unique_whats_domain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"n_whats_filtered\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=1,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('number of steps in instructions')\n",
    "plt.savefig('./plots/instruction_steps_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_whats_filtered = df_trial[df_trial.domain=='drawing']['n_whats_filtered']\n",
    "s_whats_filtered = df_trial[df_trial.domain=='structures']['n_whats_filtered']\n",
    "\n",
    "stats.ttest_ind(d_whats_filtered,s_whats_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_whats_filtered.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_whats_filtered.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bfa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "\n",
    "plt.figure(figsize=(4,6))\n",
    "sns.barplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='domain', \n",
    "             y='n_unique_whats')\n",
    "plt.ylabel('unique what words')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response')\n",
    "plt.savefig('./plots/unique_whats_domain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69020f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial, \n",
    "             x=\"n_unique_whats\", \n",
    "             hue=\"domain\",\n",
    "             hue_order=['drawing','structures'],\n",
    "             binwidth=1,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('number of steps in instructions')\n",
    "plt.savefig('./plots/instruction_steps_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ecad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_unique_whats = df_trial[df_trial.domain=='drawing']['n_unique_whats']\n",
    "s_unique_whats = df_trial[df_trial.domain=='structures']['n_unique_whats']\n",
    "\n",
    "stats.ttest_ind(d_unique_whats,s_unique_whats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='subdomain', \n",
    "             y='n_unique_whats',\n",
    "             hue='domain')\n",
    "plt.ylabel('unique what words')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response')\n",
    "plt.savefig('./plots/unique_whats_subdomain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f185e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over time\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df_trial[(df_trial.complete_dataset)], \n",
    "             x='trial_num', \n",
    "             y='n_unique_whats', \n",
    "             hue='domain')\n",
    "# plt.ylim((0,275))\n",
    "plt.title('number of unique words used per response, over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd625cc",
   "metadata": {},
   "source": [
    "#### Comparisons between subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19208116",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,8))\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "            data = df_trial,\n",
    "            x = 'subdomain',\n",
    "            order = subdomains['drawing'] +  subdomains['structures'],\n",
    "            palette= {**domain_palettes['drawing'],**domain_palettes_light['structures']},\n",
    "            y = 'what_word_sum')\n",
    "_ = plt.xticks(rotation = 60)\n",
    "plt.savefig('./plots/what_word_sum_subdomains.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.violinplot(\n",
    "    data = df_trial,\n",
    "    x = 'subdomain',\n",
    "    order = subdomains['drawing'] +  subdomains['structures'],\n",
    "    y = 'what_word_sum',\n",
    "    palette= {**domain_palettes['drawing'],**domain_palettes_light['structures']},\n",
    "    linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.violinplot(\n",
    "    data = df_trial.groupby(['domain','subdomain','stimId']).first().reset_index(),\n",
    "    x = 'subdomain',\n",
    "    order = subdomains['drawing'] +  subdomains['structures'],\n",
    "    palette = {**domain_palettes['drawing'],**domain_palettes_light['structures']},\n",
    "    y = 'what_word_mean',\n",
    "    linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f26d1",
   "metadata": {},
   "source": [
    "### Word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a916280",
   "metadata": {},
   "source": [
    "token-based length: across domains, across subdomains within domain, across stims within subdomain, across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_trial, x=\"what_word_sum\", log_scale=True, fill=False, element=\"step\")\n",
    "sns.histplot(data = df_trial, x=\"where_word_sum\", log_scale=True, fill=False, element=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493182b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_trial,\n",
    "             x=\"what_word_sum\", \n",
    "             hue='domain',\n",
    "             stat=\"density\",\n",
    "             common_norm=False,\n",
    "             log_scale=True,\n",
    "             fill=False,\n",
    "             element=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac167db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean word count (across participants) for each subdomain\n",
    "plt.figure(figsize=(4,6))\n",
    "\n",
    "sns.barplot(data=what_word_sum_means,\n",
    "            x='domain',\n",
    "            y='what_word_mean',\n",
    "            hue_order=['structures','drawing']\n",
    "           )\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('what words')\n",
    "plt.title('word count by domain')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.subplots_adjust(left=0.2, bottom=0.35)\n",
    "plt.savefig('./plots/what_word_count_domain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f424745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean word count (across participants) for each subdomain\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "sns.barplot(data=what_word_sum_means,\n",
    "            x='subdomain',\n",
    "            hue='domain',\n",
    "            y='what_word_mean',\n",
    "            hue_order=['drawing','structures']\n",
    "           )\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('what words')\n",
    "plt.title('word count by subdomain')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.subplots_adjust(left=0.2, bottom=0.35)\n",
    "plt.savefig('./plots/what_word_count_subdomain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df_trial[df_trial.subdomain.isin(['nuts-bolts','dials'])], \n",
    "             x=\"what_word_mean\", \n",
    "             hue=\"subdomain\",\n",
    "#              hue_order=['drawing','structures'],\n",
    "             binwidth=5,\n",
    "             stat='proportion')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.title('mean number of words in instructions')\n",
    "plt.savefig('./plots/instruction_chars_dist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc9059",
   "metadata": {},
   "source": [
    "## Top words in domain (by counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc78efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by domain\n",
    "all_words = {}\n",
    "top_words_domain = {}\n",
    "n_words_in_domain = {}\n",
    "tf = {}\n",
    "df = {}\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for domain in domains:\n",
    "    \n",
    "    doc = [d for sublist in df_trial[(df_trial.domain==domain) &\n",
    "                                     (df_trial.complete_dataset)]['lemmatized_filtered_whats'] \n",
    "                                              for item in sublist\n",
    "                                              for d in item]\n",
    "    \n",
    "    doc = [word for word in doc if word_to_pos[word]!='NUM']\n",
    "    c = Counter(doc)\n",
    "    top_words_domain[domain] = c.most_common(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee57224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_words = pd.DataFrame(top_words_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_OFFSET_START = 0\n",
    "Y_OFFSET_START = 1\n",
    "\n",
    "X_OFFSET_INTERVAL = 0.5\n",
    "Y_OFFSET_INTERVAL = 0.1\n",
    "\n",
    "X_OFFSET_WORD = 0.13\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"Greys\")\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "x_offset = X_OFFSET_START\n",
    "for domain in domains:\n",
    "    \n",
    "    y_offset = Y_OFFSET_START\n",
    "\n",
    "#     plt.text(x_offset, y_offset, domain, fontweight=\"bold\", color=cmap(1.0))\n",
    "    y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    domain_counts = np.array([b for (a, b) in top_words_domain[domain]])\n",
    "    \n",
    "    norm = matplotlib.colors.Normalize(vmin=min(-(domain_counts.mean()*3), domain_counts.min()), vmax=domain_counts.max())\n",
    "    \n",
    "    for word, count in top_words_domain[domain]:\n",
    "        alpha = 1\n",
    "        plt.text(x_offset, y_offset, f\"({count:.0f}) \", color=cmap(norm(count)), fontsize=12, alpha=alpha, fontname=\"Arial\")\n",
    "        plt.text(x_offset + X_OFFSET_WORD, y_offset, word, color=cmap(norm(count)), fontsize=16, alpha=alpha)\n",
    "        \n",
    "        y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    x_offset += X_OFFSET_INTERVAL\n",
    "    \n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"top_words.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce99ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Most diagnostic words of subdomain (PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = \"structures\"\n",
    "# DOMAIN = \"drawing\"\n",
    "\n",
    "df_domain = df_trial[(df_trial.domain == DOMAIN) & (df_trial.complete_dataset) & (~df_trial.ppt_hit_8_step_limit) & (df_trial.stimId != 'demo_stim')]\n",
    "df_domain = df_domain.reset_index(drop=True)\n",
    "\n",
    "df_domain[\"lemmatized_whats_flat\"] = df_domain[\"lemmatized_whats\"].map(lambda whats_list: \" \".join([spelling_map[item] for sublist in whats_list for item in sublist if word_to_pos[item] == \"NOUN\"]))\n",
    "\n",
    "\n",
    "df_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents=\"unicode\", min_df=5, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df_domain[\"lemmatized_whats_flat\"])\n",
    "\n",
    "df_counts = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "PSEUDOCOUNT = 1 / len(df_counts.columns)\n",
    "\n",
    "df_counts = df_counts + PSEUDOCOUNT\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pmi = defaultdict(dict)\n",
    "\n",
    "N = df_counts.sum().sum()\n",
    "JOINT_EXP = 1\n",
    "\n",
    "subdomain_priors = ((df_domain.subdomain.value_counts()) / len(df_domain)).to_dict()\n",
    "\n",
    "for subdomain in subdomain_priors:\n",
    "    p_subdomain = np.log2(subdomain_priors[subdomain])\n",
    "    for word in df_counts.columns:\n",
    "        p_joint = np.log2((df_counts[word][df_domain.subdomain == subdomain].sum()) / N)\n",
    "        p_word = np.log2((df_counts[word].sum()) / N)\n",
    "                \n",
    "        pmi = p_joint - (p_word + p_subdomain)\n",
    "        d_pmi[subdomain][word] = pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of positive PMI values\n",
    "df_pmi = pd.DataFrame(d_pmi)\n",
    "(df_pmi > 0).sum().sum() / (df_pmi != 0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({subdomain: df_pmi[subdomain].nlargest(30).index.tolist() for subdomain in subdomain_priors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "df_pmi = df_pmi[subdomains[DOMAIN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb879405",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10\n",
    "\n",
    "X_OFFSET_START = 0\n",
    "Y_OFFSET_START = 1\n",
    "\n",
    "X_OFFSET_INTERVAL = 0.35\n",
    "Y_OFFSET_INTERVAL = 0.1\n",
    "\n",
    "X_OFFSET_WORD = 0.11\n",
    "\n",
    "# CMAPS = map(matplotlib.cm.get_cmap, [\"Blues\", \"Oranges\", \"Greens\", \"Reds\"])\n",
    "CMAPS = gradients\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "x_offset = X_OFFSET_START\n",
    "for subdomain, cmap in zip(df_pmi.columns, CMAPS):\n",
    "    df_pmi_subdomain_top = df_pmi[subdomain].nlargest(TOP_N)\n",
    "    \n",
    "    y_offset = Y_OFFSET_START\n",
    "\n",
    "#     plt.text(x_offset, y_offset, subdomain, fontweight=\"bold\", color=cmap(1.0), fontname=\"Arial\")\n",
    "    y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    norm = matplotlib.colors.Normalize(vmin=min(0, df_pmi_subdomain_top.min()), vmax=df_pmi_subdomain_top.max())\n",
    "    \n",
    "    for word, pmi in df_pmi_subdomain_top.iteritems():\n",
    "        alpha = int(pmi > 0)\n",
    "        plt.text(x_offset, y_offset, f\"({pmi:.2f}) \", color=cmap(norm(pmi)), fontsize=12, alpha=alpha, fontname=\"Arial\")\n",
    "        plt.text(x_offset + X_OFFSET_WORD, y_offset, word, color=cmap(norm(pmi)), fontsize=16, alpha=alpha)\n",
    "        \n",
    "        y_offset -= Y_OFFSET_INTERVAL\n",
    "    \n",
    "    x_offset += X_OFFSET_INTERVAL\n",
    "    \n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"pmi_{DOMAIN}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd80b4",
   "metadata": {},
   "source": [
    "## Analysis of programs x language\n",
    "\n",
    "- Scatterplot: for each stim AND baseDSL vs. ‘compressive’ DSLL: word count vs. program length; |set(words)| vs.  |set(program_tokens)|\n",
    "  - Questions re: overall trend (sublinear?)\n",
    "  - Questions re: where we see high variation in language length given baseDSL program length \n",
    "  - We can further break these down in: domain, subdomain, annotator, library_0 vs. library_compressive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781219cb",
   "metadata": {},
   "source": [
    "Let's merge the df_structures data frame with trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = sns.scatterplot(x='n_blocks', y = 'what_char_sum', alpha=0.5, data=df_combined)\n",
    "g = sns.jointplot(x='n_blocks', y = 'what_word_sum', alpha = 0.2,  data=df_trial)\n",
    "#g.ax_joint.set_xscale('log')\n",
    "#g.ax_joint.set_yscale('log')\n",
    "#fig.plot([0, 100], [0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef92e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.query('what_word_sum == 1')['responses']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360bcbc",
   "metadata": {},
   "source": [
    "### n stroke vs mean word count for gadget item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9293124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'drawing'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'n_strokes',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "# plt.title(\"number of words by gadget complexity\")\n",
    "plt.xlabel(\"strokes in image\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "s.plot([0,1],[0,1], \n",
    "       transform=s.transAxes, \n",
    "       color='grey',\n",
    "       linestyle='--')\n",
    "\n",
    "plt.savefig('./plots/gadget_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ee4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df = df_trial[df_trial.domain == 'drawing'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.85, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'n_strokes',\n",
    "        'what_word_mean',\n",
    "        alpha=0.6)\n",
    "\n",
    "\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "    ax.set(xlabel=\"strokes in image\", ylabel=\"mean number of words\")\n",
    "    \n",
    "plt.savefig('./plots/gadget_language_item_complexity_facet.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c8e53",
   "metadata": {},
   "source": [
    "### n blocks vs mean word count for structure item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9175ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'structures'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'n_blocks',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"blocks in structure\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "s.plot([0,1],[0,1], \n",
    "       transform=s.transAxes, \n",
    "       color='grey',\n",
    "       linestyle='--')\n",
    "\n",
    "plt.savefig('./plots/structure_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df = df_trial[df_trial.domain == 'structures'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.85, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'n_blocks',\n",
    "        'what_word_mean',\n",
    "        alpha=0.6)\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "    ax.set(xlabel=\"blocks in structure\", ylabel=\"mean number of words\")\n",
    "    \n",
    "plt.savefig('./plots/structure_language_item_complexity_facet.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18c42c",
   "metadata": {},
   "source": [
    "### base program length vs mean word count for gadget item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'drawing'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'base_program_length',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"base program length\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "# s.plot([0,1],[0,1], \n",
    "#        transform=s.transAxes, \n",
    "#        color='grey',\n",
    "#        linestyle='--')\n",
    "\n",
    "plt.savefig('./plots/gadget_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a793ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "# remove one outlier to scale graphs\n",
    "df = df_trial[(df_trial.domain == 'drawing') & (df_trial.what_word_mean < 125)].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.8, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                  sharex=False,\n",
    "                  sharey=True,\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'base_program_length',\n",
    "        'what_word_mean',\n",
    "        alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.set_title(None)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xlabel(None)\n",
    "#     ax.set_xlim([0,470])\n",
    "\n",
    "# for ax in s.axes_dict.values():\n",
    "#     ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "#     ax.set(xlabel=\"base_program_length\", ylabel=\"mean number of words\")\n",
    "    \n",
    "plt.savefig('./plots/gadget_basedsl_langlength_facet.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101599f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdomain in df[\"subdomain\"].unique():\n",
    "    print(subdomain)\n",
    "    df[df[\"subdomain\"] == subdomain]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb96343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"subdomain\"] == subdomain][[\"base_program_length\", \"what_word_mean\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(\n",
    "    x=df[df[\"subdomain\"] == subdomain][\"base_program_length\"],\n",
    "    y=df[df[\"subdomain\"] == subdomain][\"what_word_mean\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e2414",
   "metadata": {},
   "source": [
    "### n blocks vs mean word count for structure item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df =  df_trial[df_trial.domain == 'structures'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                x = 'base_program_length',\n",
    "                y = 'what_word_mean',\n",
    "                hue='subdomain',\n",
    "                alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"base_program_length\")\n",
    "plt.ylabel(\"mean number of words\")\n",
    "\n",
    "# s.plot([0,1],[0,1], \n",
    "#        transform=s.transAxes, \n",
    "#        color='grey',\n",
    "#        linestyle='--')\n",
    "\n",
    "plt.savefig('./plots/structure_language_item_complexity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1408b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df = df_trial[(df_trial.domain == 'structures') & (df_trial.what_word_mean < 150)].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "s = sns.FacetGrid(data = df,\n",
    "                  col='subdomain', \n",
    "                  hue='subdomain',\n",
    "                  height=5, \n",
    "                  aspect=0.8, # set aspect ratio here (although this includes titles, labels etc.)\n",
    "                  sharex=False,\n",
    "                  sharey=True,\n",
    "                )\n",
    "\n",
    "s.map(sns.scatterplot,\n",
    "        'base_program_length',\n",
    "        'what_word_mean',\n",
    "        alpha=0.5)\n",
    "\n",
    "for ax in s.axes_dict.values():\n",
    "    ax.set_title(None)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xlabel(None)\n",
    "#     ax.set_xlim([0,470])\n",
    "\n",
    "# for ax in s.axes_dict.values():\n",
    "#     ax.axline((0, 0), slope=1, c=\".3\", ls=\"--\", zorder=0)\n",
    "#     ax.set(xlabel=\"base_program_length\", ylabel=\"mean number of words\")\n",
    "    \n",
    "plt.savefig('./plots/structures_basedsl_langlength_facet.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9c766",
   "metadata": {},
   "source": [
    "## Likelihood ratio test for linear vs. log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.statology.org/likelihood-ratio-test-in-python/\n",
    "\n",
    "likelihood_test_results = []\n",
    "\n",
    "for domain in [\"drawing\", \"structures\"]:\n",
    "    df = df_trial[df_trial.domain == domain].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "    for subdomain in df[\"subdomain\"].unique():\n",
    "        y = df[df[\"subdomain\"] == subdomain][\"what_word_mean\"]\n",
    "\n",
    "        # Reduced model\n",
    "        x = df[df[\"subdomain\"] == subdomain][\"base_program_length\"]\n",
    "        x = sm.add_constant(x)\n",
    "        reduced_model = sm.OLS(y, x).fit()\n",
    "\n",
    "        # Full model\n",
    "        df[\"log_base_program_length\"] = np.log(df[\"base_program_length\"])\n",
    "\n",
    "        x = df[df[\"subdomain\"] == subdomain][[\"base_program_length\", \"log_base_program_length\"]]\n",
    "        x = sm.add_constant(x)\n",
    "        full_model = sm.OLS(y, x).fit()\n",
    "\n",
    "        #calculate likelihood ratio Chi-Squared test statistic\n",
    "        LR_statistic = -2*(reduced_model.llf - full_model.llf)\n",
    "\n",
    "        #calculate p-value of test statistic using 2 degrees of freedom\n",
    "        p_val = scipy.stats.chi2.sf(LR_statistic, 2)\n",
    "\n",
    "        likelihood_test_results.append({\n",
    "            \"domain\": domain,\n",
    "            \"subdomain\": subdomain,\n",
    "            \"chi-squared\": LR_statistic,\n",
    "            \"p_val\": p_val,\n",
    "        })\n",
    "        \n",
    "df_likelihood_test_results = pd.DataFrame(likelihood_test_results)\n",
    "df_likelihood_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331172a4",
   "metadata": {},
   "source": [
    "### Compare distributions of words across domain/ subdomains (JSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ee90f",
   "metadata": {},
   "source": [
    "#### Between domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COUNTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_whats = df_trial.groupby(['gameID','trial_num'])['lemmatized_filtered_whats'].apply(lambda trial_responses: \\\n",
    "    ([x for xs in [word for sublist in trial_responses for word in sublist] for x in xs]))\n",
    "trial_whats_counts = trial_whats.apply(lambda x: Counter(x))\n",
    "\n",
    "df_trial_whats = df_trial[['gameID','trial_num','subdomain', 'domain','lemmatized_filtered_whats']].groupby(['gameID','trial_num']).first()\n",
    "df_trial_whats.loc[:,'trial_whats'] = trial_whats\n",
    "df_trial_whats.loc[:,'what_counts'] = trial_whats_counts\n",
    "\n",
    "all_words = np.unique([x for xs in trial_whats for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceba6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in all_words:\n",
    "    if USE_COUNTS:\n",
    "        df_trial_whats[w] = df_trial_whats['what_counts'].apply(lambda row: int(row[w])) # word counts\n",
    "    else:\n",
    "        df_trial_whats[w] = df_trial_whats['trial_whats'].apply(lambda row: int(w in row)) # present/absent\n",
    "        \n",
    "df_trial_whats = df_trial_whats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate true JSD\n",
    "\n",
    "# word counts for domains\n",
    "drawing_counts = df_trial_whats.loc[(df_trial_whats['domain'] == 'drawing')].iloc[:,10:].sum(axis=0)\n",
    "structures_counts = df_trial_whats.loc[(df_trial_whats['domain'] == 'structures')].iloc[:,10:].sum(axis=0)\n",
    "\n",
    "true_jsd = distance.jensenshannon(drawing_counts,structures_counts,2)\n",
    "print(true_jsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null distribution of JSDs\n",
    "# JSD for distributions of words in domains\n",
    "# Shuffle domain tags. 1000 random assingments to 2 (preserve sizes)\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "n_iters = 1000\n",
    "\n",
    "jsds = []\n",
    "\n",
    "# calculate true split of trials into domains\n",
    "domain_assignments = df_trial.domain.copy()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# for each iteration\n",
    "for n in range(0, n_iters):\n",
    "    \n",
    "    # assign trial random domain tag (following partition of domains in data)\n",
    "    np.random.shuffle(domain_assignments)\n",
    "    \n",
    "    drawing_counts = df_trial_whats.iloc[:,10:][domain_assignments == 'drawing'].sum(axis = 0)\n",
    "    structures_counts = df_trial_whats.iloc[:,10:][domain_assignments == 'structures'].sum(axis = 0)\n",
    "    jsd = distance.jensenshannon(drawing_counts,structures_counts,2)\n",
    "    \n",
    "    # calculate JSD\n",
    "    jsds.append(jsd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a716c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(jsds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report p-value. (how many are greater than true JSD)\n",
    "(sum(jsds > true_jsd) / n_iters) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b03f4",
   "metadata": {},
   "source": [
    "#### Between subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'drawing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate true mean JSD\n",
    "# One domain at a time\n",
    "df_trial_whats_domain = df_trial_whats.loc[(df_trial_whats['domain'] == domain)].reset_index(drop=True).copy()\n",
    "\n",
    "subdomain_counts = {}\n",
    "subdomain_jsds = {}\n",
    "\n",
    "# get counts\n",
    "for subdomain in subdomains[domain]:\n",
    "    subdomain_counts[subdomain] = df_trial_whats_domain.loc[(df_trial_whats['subdomain'] == subdomain)]\\\n",
    "                                    .iloc[:,10:].sum(axis=0)\n",
    "\n",
    "# get JSDS\n",
    "for subdomain_i in subdomains[domain]:\n",
    "    subdomain_jsds[subdomain_i] = {}\n",
    "    for subdomain_j in subdomains[domain]:\n",
    "        subdomain_jsds[subdomain_i][subdomain_j] = distance.jensenshannon(subdomain_counts[subdomain_i],\n",
    "                                                                          subdomain_counts[subdomain_j], 2)\n",
    "        \n",
    "\n",
    "true_subdomain_jsds = pd.DataFrame.from_dict(subdomain_jsds)\n",
    "true_subdomain_jsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true mean JSD\n",
    "true_mean_jsd = np.array(true_subdomain_jsds)[np.triu_indices(4,k = 1)].mean()\n",
    "true_mean_jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db551ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null distribution of mean JSDs\n",
    "\n",
    "df_trial_whats_domain = df_trial_whats.loc[(df_trial_whats['domain'] == domain)].reset_index(drop=True).copy()\n",
    "\n",
    "mean_jsds = []\n",
    "\n",
    "n_iters = 1000\n",
    "\n",
    "# calculate true split of trials into domains\n",
    "subdomain_assignments = df_trial_whats_domain.subdomain.copy()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "for n in range(0,n_iters):\n",
    "    \n",
    "    np.random.shuffle(subdomain_assignments)\n",
    "\n",
    "    subdomain_counts = {}\n",
    "    subdomain_jsds = {}\n",
    "\n",
    "    # get counts\n",
    "    for subdomain in subdomains[domain]:\n",
    "        subdomain_counts[subdomain] = df_trial_whats_domain.iloc[:,10:][subdomain_assignments == subdomain]\\\n",
    "                                        .sum(axis=0)\n",
    "\n",
    "    # get JSDS\n",
    "    for subdomain_i in subdomains[domain]:\n",
    "        subdomain_jsds[subdomain_i] = {}\n",
    "        for subdomain_j in subdomains[domain]:\n",
    "            subdomain_jsds[subdomain_i][subdomain_j] = distance.jensenshannon(subdomain_counts[subdomain_i],\n",
    "                                                                              subdomain_counts[subdomain_j], 2)\n",
    "\n",
    "\n",
    "    subdomain_jsds = pd.DataFrame.from_dict(subdomain_jsds)\n",
    "    mean_jsd = np.array(subdomain_jsds)[np.triu_indices(4,k = 1)].mean()\n",
    "    mean_jsds.append(mean_jsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_jsds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a697650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report p-value. (how many are greater than true JSD)\n",
    "(sum(mean_jsds > true_mean_jsd) / n_iters) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c36c23",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378d11c",
   "metadata": {},
   "source": [
    "### merge urls with top down abstraction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = df_trial[df_trial.domain=='structures'].groupby(['stimId','stimURL','subdomain']).first().reset_index()[['blocks','stimId','stimURL','domain','subdomain']]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topdownabs = pd.read_csv('../../stimuli/towers/df_structures_topdownabs_consistent_abstractions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5decb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topdownabs['stimId'] = df_topdownabs['structure_number']\n",
    "df_topdownabs['subdomain'] = df_topdownabs['structure_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c7ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topdownabs.merge(urls, how='left',on=['blocks','stimId','subdomain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8817b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topdownabs.to_csv('../../stimuli/towers/df_structures_topdownabs_consistent_abstractions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6236c01",
   "metadata": {},
   "source": [
    "# Exploratory analyses that do not appear in cogsci 22 paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9486f8",
   "metadata": {},
   "source": [
    "## Comparisons between domains/ subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e30baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COUNTS = True # if not, use present/absent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b76f29",
   "metadata": {},
   "source": [
    "### get word-frequency vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d6ea1",
   "metadata": {},
   "source": [
    "#### by participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_whats = df_trial.groupby('gameID')['lemmatized_filtered_whats'].apply(lambda ppt_responses: \\\n",
    "    ([x for xs in [word for sublist in ppt_responses for word in sublist] for x in xs]))\n",
    "ppt_whats_counts = ppt_whats.apply(lambda x: Counter(x))\n",
    "\n",
    "df_ppts_whats = df_trial[['gameID', 'subdomain', 'domain','lemmatized_filtered_whats']].groupby('gameID').first()\n",
    "df_ppts_whats.loc[:,'ppt_whats'] = ppt_whats\n",
    "df_ppts_whats.loc[:,'what_counts'] = ppt_whats_counts\n",
    "\n",
    "all_words = np.unique([x for xs in ppt_whats for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838564f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in all_words:\n",
    "    if USE_COUNTS:\n",
    "        df_ppts_whats[w] = df_ppts_whats['what_counts'].apply(lambda row: int(row[w])) # word counts\n",
    "    else:\n",
    "        df_ppts_whats[w] = df_ppts_whats['ppt_whats'].apply(lambda row: int(w in row)) # present/absent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de77c3",
   "metadata": {},
   "source": [
    "#### by trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_whats = df_trial.groupby(['gameID','trial_num'])['lemmatized_filtered_whats'].apply(lambda trial_responses: \\\n",
    "    ([x for xs in [word for sublist in trial_responses for word in sublist] for x in xs]))\n",
    "trial_whats_counts = trial_whats.apply(lambda x: Counter(x))\n",
    "\n",
    "df_trial_whats = df_trial[['gameID','trial_num','subdomain', 'domain','lemmatized_filtered_whats']].groupby(['gameID','trial_num']).first()\n",
    "df_trial_whats.loc[:,'trial_whats'] = trial_whats\n",
    "df_trial_whats.loc[:,'what_counts'] = trial_whats_counts\n",
    "\n",
    "all_words = np.unique([x for xs in trial_whats for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in all_words:\n",
    "    if USE_COUNTS:\n",
    "        df_trial_whats[w] = df_trial_whats['what_counts'].apply(lambda row: int(row[w])) # word counts\n",
    "    else:\n",
    "        df_trial_whats[w] = df_trial_whats['trial_whats'].apply(lambda row: int(w in row)) # present/absent\n",
    "        \n",
    "df_trial_whats = df_trial_whats.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f67672",
   "metadata": {},
   "source": [
    "#### f-statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739809e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# between-group variability\n",
    "\n",
    "# sample mean of ith group\n",
    "\n",
    "def F_stat(grouping, data, verbose = False, filter_n_1 = True):\n",
    "    '''\n",
    "    Caluclates F-statistic for a particular grouping of datapoints.\n",
    "    \n",
    "    grouping: series of labels. If using clustering, use cluster.labels_\n",
    "    data: should contain data only, with rows corresponding to the labels in grouping.\n",
    "    filter_n_1: whether or not to remove degenerate groups with counts less than 2.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    labels = list(set(grouping))\n",
    "    \n",
    "    label_counter = Counter(grouping)\n",
    "    \n",
    "#     if filter_n_1:\n",
    "#         labels = [label for label in labels if (label_counter[label] > 1)]\n",
    "#         grouping = [label for label in grouping if (label_counter[label] > 1)]\n",
    "#         data = data[list(map(lambda x: label_counter[x] > 1, grouping))]\n",
    "    \n",
    "    n_groups = len(labels)\n",
    "    \n",
    "    n_datapoints = len(data)\n",
    "    \n",
    "    overall_mean = np.mean(data, axis=0)\n",
    "    \n",
    "    total_within_variance = 0\n",
    "    \n",
    "    total_between_variance = 0\n",
    "\n",
    "    for group_label in labels:\n",
    "        # get rows of all ppts for that group\n",
    "        group_members = data.iloc[np.where(grouping==group_label)[0],:]\n",
    "        \n",
    "        # calculate group mean\n",
    "        group_mean = np.mean(group_members, axis=0)\n",
    "        \n",
    "        # between group\n",
    "        group_squared_error = np.square(distance.euclidean(group_mean, overall_mean))\n",
    "        between_group_value = (len(group_members)*group_squared_error)/(n_groups-1)\n",
    "        total_between_variance += between_group_value\n",
    "        \n",
    "        # Within group\n",
    "        errors = np.apply_along_axis(lambda row: distance.euclidean(row, group_mean), 1, group_members)\n",
    "        within_group_sum = np.sum(np.square(errors)/(n_datapoints-n_groups))\n",
    "        total_within_variance += within_group_sum\n",
    "        \n",
    "    return (total_between_variance/total_within_variance, total_between_variance, total_within_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are vocabularies different across subdomains?\n",
    "\n",
    "# By participant\n",
    "F_stat(df_ppts_whats['subdomain'],df_ppts_whats.iloc[:,6:]) # adjust hardcoded 5 to get just the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are vocabularies different across subdomains?\n",
    "\n",
    "# By participant\n",
    "F_stat(df_trial_whats.gameID, df_trial_whats.iloc[:,7:]) # adjust hardcoded 7 to get just the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad7dd9a",
   "metadata": {},
   "source": [
    "### Bootstrap F-tests (decided this was unconventional)\n",
    "\n",
    "Potentially over-accounting for large word counts (e.g. red) as these are squared (twice if using Euclidean distance)\n",
    "\n",
    "**Todo: z-score within columns (words) for whole dataset, then re-run**\n",
    "\n",
    "**Todo: run with individual trials and clusters**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36de05",
   "metadata": {},
   "source": [
    "Questions to answer with f-stats (or chisquared tests):\n",
    "\n",
    "- Do people use different words for different subdomains? (done- but use pca properly (incl. taking top pcs) and preprocessed df)\n",
    "  - The stronger version of this claim asks, *within a domain*, do people use different words? \n",
    "  - data: separate for domains \n",
    "  - df rows: ppt\n",
    "  - grouping: subdomains\n",
    "  - baseline: randomly assigned subdomains\n",
    "- Do distinct strategies exist?\n",
    "  - Does clusters have smaller f-statistic than random assignments? (seems like it obviously will)\n",
    "  - Again, I think that lumping in everything together will primarily recover ths domains, and maybe the subdomains. \n",
    "  - data: separate for domains \n",
    "  - df rows: ppt?\n",
    "  - grouping: subdomains\n",
    "  - baseline: randomly assigned subdomains\n",
    "- Are people consistent with how they use language? I.e. are their trials likely to be assigned to the same cluster than a random other cluster?\n",
    "  - likely some other (related) analysis\n",
    "  - data: separate for domains\n",
    "  - df_rows: trials\n",
    "  - grouping: ppt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6b6f8",
   "metadata": {},
   "source": [
    "#### Is language more consistent within a subdomain (compared to random assignments)?\n",
    "\n",
    "Yes overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45804be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subdomain grouping by ppt for both domains at once\n",
    "\n",
    "n_ppts = df_trial_whats['gameID'].nunique() \n",
    "ppt_indices = list(range(0,n_ppts))\n",
    "ppts =  df_trial_whats['gameID'].unique()\n",
    "nIters = 100\n",
    "nGroups = len(df_ppts_whats.subdomain.unique())\n",
    "\n",
    "f_diffs = []\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "for i in range (0, nIters):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    ppt_sample = [np.random.choice(ppt_indices, ) for _ in ppt_indices]\n",
    "    \n",
    "    ppt_sample_index = ppts[ppt_sample]\n",
    "    \n",
    "    df_sample = df_ppts_whats.loc[ppt_sample_index]\n",
    "    \n",
    "    subdomain_f = F_stat(df_sample['subdomain'], df_sample.iloc[:,5:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    random_group_assignment = [random.randint(0, nGroups) for i in ppt_indices]\n",
    "    random_sample_groups = pd.Series(random_group_assignment)[ppt_sample]\n",
    "    \n",
    "    randomized_f = F_stat(random_sample_groups, df_sample.iloc[:,5:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    f_diff = subdomain_f[0] - randomized_f[0]\n",
    "    f_diffs.append(f_diff)\n",
    "    \n",
    "f_diffs = pd.Series(f_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a671b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(f_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(f_diffs < 0) / nIters) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d54c15",
   "metadata": {},
   "source": [
    "#### Stronger test: Is language more consistent within a subdomain (compared to random assignments), comparing only within a domain?\n",
    "\n",
    "Certainly for gadgets, marginally for structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b352a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subdomain grouping by ppt for individual domain\n",
    "\n",
    "domain = 'drawing'\n",
    "df_ppt_domain = df_ppts_whats[df_ppts_whats.domain == domain]\n",
    "\n",
    "n_ppts = df_ppt_domain.index.nunique() \n",
    "ppt_indices = list(range(0,n_ppts))\n",
    "ppts =  df_ppt_domain.index.unique()\n",
    "nIters = 1000\n",
    "nGroups = len(df_ppt_domain['subdomain'].unique())\n",
    "\n",
    "f_diffs = []\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "for i in range (0, nIters):\n",
    "    \n",
    "    ppt_sample = [np.random.choice(ppt_indices) for _ in ppt_indices]\n",
    "    \n",
    "    ppt_sample_index = ppts[ppt_sample]\n",
    "    \n",
    "    df_sample = df_ppt_domain.loc[ppt_sample_index]\n",
    "    \n",
    "    subdomain_f = F_stat(df_sample['subdomain'], df_sample.iloc[:,5:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    random_group_assignment = [np.random.randint(0,nGroups) for i in ppt_indices]\n",
    "    random_sample_groups = pd.Series(random_group_assignment)[ppt_sample]\n",
    "    \n",
    "    randomized_f = F_stat(random_sample_groups, df_sample.iloc[:,5:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    f_diff = subdomain_f[0] - randomized_f[0]\n",
    "    f_diffs.append(f_diff)\n",
    "    \n",
    "f_diffs = pd.Series(f_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(f_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6086764",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9285211",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(f_diffs, [0.05, 0.95]) # 95% CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(f_diffs < 0) / nIters) * 2 # p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1aa7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is language more consistent within a subdomain (compared to random assignments)\n",
    "# subdomain grouping by ppt for both domains at once\n",
    "\n",
    "n_ppts = df_trial_whats['gameID'].nunique() \n",
    "ppt_indices = list(range(0,n_ppts))\n",
    "ppts =  df_trial_whats['gameID'].unique()\n",
    "nIters = 100\n",
    "nGroups = len(df_ppts_whats.subdomain.unique())\n",
    "\n",
    "f_diffs = []\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "for i in range (0, nIters):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    ppt_sample = [np.random.choice(ppt_indices, ) for _ in ppt_indices]\n",
    "    \n",
    "    ppt_sample_index = ppts[ppt_sample]\n",
    "    \n",
    "    df_sample = df_ppts_whats.loc[ppt_sample_index]\n",
    "    \n",
    "    subdomain_f = F_stat(df_sample['subdomain'], df_sample.iloc[:,5:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    random_group_assignment = [random.randint(0,nGroups) for i in ppt_indices]\n",
    "    random_sample_groups = pd.Series(random_group_assignment)[ppt_sample]\n",
    "    \n",
    "    randomized_f = F_stat(random_sample_groups, df_sample.iloc[:,5:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    f_diff = subdomain_f[0] - randomized_f[0]\n",
    "    f_diffs.append(f_diff)\n",
    "    \n",
    "f_diffs = pd.Series(f_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### todo: are individual participants consistent with their words (more so than chance within a subdomain)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ccf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by individual trial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working. by individual trial\n",
    "\n",
    "domain = 'structures'\n",
    "df_trial_domain = df_trial_whats[df_trial_whats.domain == domain]\n",
    "\n",
    "n_ppts = df_trial_domain.gameID.nunique() \n",
    "ppt_indices = list(range(0,n_ppts))\n",
    "ppts =  df_trial_domain.gameID.unique()\n",
    "nIters = 100\n",
    "nGroups = len(df_trial_domain['subdomain'].unique())\n",
    "\n",
    "f_diffs = []\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "for i in range (0, nIters):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    ppt_sample = [np.random.choice(ppt_indices, ) for _ in ppt_indices]\n",
    "    \n",
    "    ppt_sample_index = ppts[ppt_sample]\n",
    "    \n",
    "    df_sample = df_trial_domain.loc[ppt_sample_index]\n",
    "    \n",
    "    subdomain_f = F_stat(df_sample['subdomain'], df_sample.iloc[:,7:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    random_group_assignment = [random.randint(0,nGroups) for i in ppt_indices]\n",
    "    random_sample_groups = pd.Series(random_group_assignment)[ppt_sample]\n",
    "    \n",
    "    randomized_f = F_stat(random_sample_groups, df_sample.iloc[:,7:]) # adjust hardcoded value to get just the data\n",
    "    \n",
    "    f_diff = subdomain_f[0] - randomized_f[0]\n",
    "    f_diffs.append(f_diff)\n",
    "    \n",
    "f_diffs = pd.Series(f_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfafcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945edfcd",
   "metadata": {},
   "source": [
    "### Tsne plots to visualize distinct word use across (domain, subdomain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669db072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd928a30",
   "metadata": {},
   "source": [
    "#### How separable are subdomains by word counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d00954",
   "metadata": {},
   "source": [
    "K-means works very well here- perhaps there's a way of systematically choosing a K that best matches the subdomain split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d351a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PCA = True\n",
    "N_COMPONENTS = 30\n",
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac11a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we identify distinct strategies in a top-down way?\n",
    "domain = 'drawing'\n",
    "\n",
    "df_clustering_subset = df_ppts_whats[df_ppts_whats.domain==domain].copy()\n",
    "X = df_clustering_subset.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a07c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=N_COMPONENTS, random_state=RANDOM_SEED)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=RANDOM_SEED)\n",
    "\n",
    "X_embedded = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abf9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {'axes.grid' : False, 'axes.linewidth': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7171d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = domain_palettes[domain].copy()\n",
    "palette[subdomains[domain][2]] = domain_palettes_dark[domain][subdomains[domain][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ce98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color by participant\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(X_embedded[:,0], \n",
    "                X_embedded[:,1], \n",
    "                hue=df_clustering_subset['subdomain'],\n",
    "                hue_order=subdomains[domain],\n",
    "                palette=palette,\n",
    "                legend='full',\n",
    "                alpha=0.9,\n",
    "                s = 160)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    left=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "    labelleft=False)\n",
    "plt.savefig('plots/gadgets_subdomain_tsne.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb8111",
   "metadata": {},
   "source": [
    "#### Can we recreate subdomains bottom up by clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run clustering\n",
    "trial_clustering = AffinityPropagation(random_state=RANDOM_SEED, damping=0.88).fit(X)\n",
    "# trial_clustering = KMeans(n_clusters=6).fit(X)\n",
    "\n",
    "df_clustering_subset['cluster_label'] = trial_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037524ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that clustering worked\n",
    "np.unique(trial_clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clusters using tsne\n",
    "\n",
    "MIN_CLUSTER_MEMBERS = 6 # color clusters with more than this many members\n",
    "\n",
    "n = int(len(X_embedded[:,0]))\n",
    "cluster_palette = np.array(sns.color_palette(\"bright\", len(set(trial_clustering.labels_))))\n",
    "cluster_palette[[(Counter(trial_clustering.labels_)[x] <= MIN_CLUSTER_MEMBERS) for x in set(trial_clustering.labels_)]] = (0.8,0.8,0.8)\n",
    "cluster_palette[[(Counter(trial_clustering.labels_)[x] > MIN_CLUSTER_MEMBERS) for x in set(trial_clustering.labels_)]] = \\\n",
    "    sns.color_palette(\"bright\", len(set(trial_clustering.labels_)) - sum([(Counter(trial_clustering.labels_)[x] <= MIN_CLUSTER_MEMBERS) for x in set(trial_clustering.labels_)]))\n",
    "\n",
    "cluster_palette = list(cluster_palette)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(X_embedded[:,0], \n",
    "                X_embedded[:,1], \n",
    "                hue=trial_clustering.labels_,\n",
    "                palette=cluster_palette, \n",
    "                legend='full',\n",
    "                alpha=0.8,\n",
    "#                 palette=palette,\n",
    "                s = 140,\n",
    "                linewidth=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f05eae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(trial_clustering.labels_).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270162f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df_clustering_subset[df_clustering_subset['cluster_label'] == 34].ppt_whats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1829733",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 34].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52801f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 6].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 16].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 25].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7d0c2",
   "metadata": {},
   "source": [
    "#### structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we identify distinct strategies in a top-down way?\n",
    "domain = 'structures'\n",
    "\n",
    "df_clustering_subset = df_ppts_whats[df_ppts_whats.domain==domain].copy()\n",
    "X = df_clustering_subset.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd76469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=N_COMPONENTS, random_state=RANDOM_SEED)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a458f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=RANDOM_SEED)\n",
    "X_embedded = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4937665",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = domain_palettes[domain].copy()\n",
    "palette[subdomains[domain][2]] = domain_palettes_dark[domain][subdomains[domain][2]]\n",
    "# palette[subdomains[domain][3]] = domain_palettes_light[domain][subdomains[domain][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf642c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color by participant\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(X_embedded[:,0], \n",
    "                X_embedded[:,1], \n",
    "                hue=df_clustering_subset['subdomain'],\n",
    "                hue_order=subdomains[domain],\n",
    "                palette=palette,\n",
    "                legend='full',\n",
    "                alpha=0.9,\n",
    "                s = 160)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    left=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "    labelleft=False)\n",
    "plt.subplots_adjust()\n",
    "plt.savefig('plots/structure_subdomain_tsne.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcc696",
   "metadata": {},
   "source": [
    "#### Can we recreate subdomains bottom up by clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run clustering\n",
    "trial_clustering = AffinityPropagation(random_state=RANDOM_SEED, damping=0.88).fit(X)\n",
    "df_clustering_subset['cluster_label'] = trial_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that clustering worked\n",
    "np.unique(trial_clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5429d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clusters using tsne\n",
    "\n",
    "MIN_CLUSTER_MEMBERS = 6 # color clusters with more than this many members\n",
    "\n",
    "n = int(len(X_embedded[:,0]))\n",
    "cluster_palette = np.array(sns.color_palette(\"jet_r\", len(set(trial_clustering.labels_))))\n",
    "cluster_palette[[(Counter(trial_clustering.labels_)[x] <= MIN_CLUSTER_MEMBERS) for x in set(trial_clustering.labels_)]] = (0.8,0.8,0.8)\n",
    "cluster_palette[[(Counter(trial_clustering.labels_)[x] > MIN_CLUSTER_MEMBERS) for x in set(trial_clustering.labels_)]] = \\\n",
    "    sns.color_palette(\"bright\", len(set(trial_clustering.labels_)) - sum([(Counter(trial_clustering.labels_)[x] <= MIN_CLUSTER_MEMBERS) for x in set(trial_clustering.labels_)]))\n",
    "\n",
    "cluster_palette = list(cluster_palette)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(X_embedded[:,0], \n",
    "                X_embedded[:,1], \n",
    "                hue=trial_clustering.labels_,\n",
    "                palette=cluster_palette, \n",
    "                legend='full',\n",
    "                alpha=0.7,\n",
    "                s = 140, \n",
    "                linewidth=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd060a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(trial_clustering.labels_).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59840dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df_clustering_subset[df_clustering_subset['cluster_label'] == 9].ppt_whats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db316052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 9].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 1].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8df982",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 4].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e43e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([x for xs in list(df_clustering_subset[df_clustering_subset['cluster_label'] == 29].ppt_whats) for x in xs])\\\n",
    "    .most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73a251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PCA = True # this might be doing the same thing as the init='pca' option for TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699014cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "\n",
    "X = df_ppts_whats.iloc[:,5:]\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=50)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "tsne = TSNE()\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "# cluster_labels = r0_clustering_original.labels_\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=df_ppts_whats['domain'], legend='full', palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "\n",
    "# palette = sns.color_palette(\"bright\", len(cluster_labels))\n",
    "# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=cluster_labels, legend='full', palette='jet_r')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975730be",
   "metadata": {},
   "source": [
    "### clustering to identify strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163d5c2",
   "metadata": {},
   "source": [
    "#### clusters across entire dataset\n",
    "Can we recover subdomains by clustering?\n",
    "\n",
    "Options:\n",
    "- word count vs. word present/absent\n",
    "- trial/ ppt\n",
    "- pca, n_components\n",
    "- cluster type, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f62c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_subset = df_trial_whats.copy()\n",
    "X = df_clustering_subset.iloc[:,7:]\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=50)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_clustering = KMeans(n_clusters=16).fit(X)\n",
    "df_clustering_subset['cluster_label'] = trial_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_clustering = AffinityPropagation(random_state=0, damping=0.8).fit(X)\n",
    "# df_clustering_subset['cluster_label'] = trial_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d802c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variance explained by each component\n",
    "plt.plot(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique labels\n",
    "np.unique(trial_clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7671393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tsne for visualization\n",
    "\n",
    "tsne = TSNE()\n",
    "X_embedded = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b971abfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize data by subdomain\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 8)\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=df_clustering_subset['subdomain'], hue_order=subdomains['structures'] + subdomains['drawing'], legend='full', alpha=0.6, palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations data by clustering\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 8)\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=df_clustering_subset['cluster_label'], legend='full', alpha=0.6, palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many in each cluster\n",
    "Counter(trial_clustering.labels_).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea70890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect one cluster\n",
    "list(df_clustering_subset[df_clustering_subset['cluster_label'] == 4].trial_whats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d0b68",
   "metadata": {},
   "source": [
    "#### Also across whole dataset, grouping responses by participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_subset = df_ppts_whats.copy()\n",
    "X = df_clustering_subset.iloc[:,5:]\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=20)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44c1c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# can we identify distinct strategies in a top-down way?\n",
    "trial_clustering = AffinityPropagation(random_state=0, damping=0.88).fit(X)\n",
    "df_clustering_subset['cluster_label'] = trial_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "\n",
    "X_embedded = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ccf4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "n = int(len(X_embedded[:,0]))\n",
    "cluster_palette = np.array(sns.color_palette(\"jet_r\", len(set(trial_clustering.labels_))))\n",
    "cluster_palette[[(Counter(trial_clustering.labels_)[x] <= 3) for x in set(trial_clustering.labels_)]] = (0.8,0.8,0.8)\n",
    "cluster_palette[[(Counter(trial_clustering.labels_)[x] > 3) for x in set(trial_clustering.labels_)]] = \\\n",
    "    sns.color_palette(\"bright\", len(set(trial_clustering.labels_)) - sum([(Counter(trial_clustering.labels_)[x] <= 3) for x in set(trial_clustering.labels_)]))\n",
    "\n",
    "cluster_palette = list(cluster_palette)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(X_embedded[:,0], \n",
    "                X_embedded[:,1], \n",
    "                hue=df_clustering_subset['cluster_label'],\n",
    "                palette=cluster_palette, \n",
    "#                 legend='full',\n",
    "                alpha=0.7,\n",
    "                s=70, \n",
    "                linewidth=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff544af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_clustering_subset[df_clustering_subset['cluster_label'] == 29].ppt_whats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 8)\n",
    "sns.scatterplot(X_embedded[:,0], \n",
    "                X_embedded[:,1], \n",
    "                hue=df_clustering_subset.subdomain,\n",
    "                hue_order=subdomains['structures'] + subdomains['drawing'],\n",
    "                palette='jet_r',\n",
    "#                 legend='full',\n",
    "                alpha=0.7,\n",
    "                s=70, \n",
    "                linewidth=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e944a4",
   "metadata": {},
   "source": [
    "### Additional tsnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e019de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "\n",
    "X = df_trial_whats.iloc[:,7:]\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=50)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "tsne = TSNE()\n",
    "\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 8)\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=df_trial_whats['gameID'], legend='full', palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "\n",
    "# palette = sns.color_palette(\"bright\", len(cluster_labels))\n",
    "# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=cluster_labels, legend='full', palette='jet_r')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd5e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PCA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50bc643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9d198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "\n",
    "X = df_trial_whats.iloc[:,7:]\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=50)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "tsne = TSNE()\n",
    "\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 8)\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=df_trial_whats['subdomain'], hue_order=subdomains['structures'] + subdomains['drawing'], legend='full', alpha=0.6, palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fec7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # visualizations using tsne\n",
    "\n",
    "# df_tmp = df_trial_whats[df_trial_whats.domain=='structures']\n",
    "\n",
    "# X = df_tmp.iloc[:,7:]\n",
    "\n",
    "# if USE_PCA:\n",
    "#     pca = PCA(n_components=50)\n",
    "#     pca.fit(X)\n",
    "#     X = pca.transform(X)\n",
    "\n",
    "# tsne = TSNE()\n",
    "\n",
    "# X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# palette = sns.color_palette(\"bright\", 4)\n",
    "sns.scatterplot(X_embedded[:,0], \n",
    "                X_embedded[:,1], \n",
    "                hue=df_tmp['gameID'], \n",
    "#                 hue_order=subdomains['structures'],\n",
    "                legend='full',\n",
    "                alpha=0.6,\n",
    "                palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59354e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "\n",
    "df_tmp = df_ppts_whats[df_ppts_whats.domain=='structures']\n",
    "\n",
    "X = df_tmp.iloc[:,5:]\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=50)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "tsne = TSNE()\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "# cluster_labels = r0_clustering_original.labels_\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 4)\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=df_tmp['subdomain'], hue_order=subdomains['structures'], legend='full', palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "\n",
    "# palette = sns.color_palette(\"bright\", len(cluster_labels))\n",
    "# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=cluster_labels, legend='full', palette='jet_r')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a57c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "\n",
    "df_tmp = df_ppts_whats[df_ppts_whats.domain=='drawing']\n",
    "\n",
    "X = df_tmp.iloc[:,5:]\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "tsne = TSNE()\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "# cluster_labels = r0_clustering_original.labels_\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 4)\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=df_tmp['subdomain'], hue_order=subdomains['drawing'], legend='full', palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "\n",
    "# palette = sns.color_palette(\"bright\", len(cluster_labels))\n",
    "# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=cluster_labels, legend='full', palette='jet_r')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne\n",
    "\n",
    "tsne = TSNE()\n",
    "X_embedded = tsne.fit_transform(df_ppts_whats[df_ppts_whats.domain=='drawing'].iloc[:,5:])\n",
    "# cluster_labels = r0_clustering_original.labels_\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], legend='full', palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "\n",
    "# palette = sns.color_palette(\"bright\", len(cluster_labels))\n",
    "# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=cluster_labels, legend='full', palette='jet_r')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2994493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games.columns[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36dfd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7f7e805",
   "metadata": {},
   "source": [
    "### Calculate tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08196354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "\n",
    "# by domain\n",
    "all_words = {}\n",
    "top_words_domain = {}\n",
    "n_words_in_domain = {}\n",
    "tf = {}\n",
    "df = {}\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for domain in domains:\n",
    "    \n",
    "    doc = [d for sublist in df_trial[(df_trial.domain==domain) &\n",
    "                                     (df_trial.complete_dataset)]['lemmatized_filtered_whats'] \n",
    "                                              for item in sublist\n",
    "                                              for d in item]\n",
    "    \n",
    "#     all_docs += doc\n",
    "    \n",
    "    c = Counter(doc)\n",
    "    top_words_domain[domain] = c.most_common(30)\n",
    "    \n",
    "    n_words_in_domain[domain] = len(c)\n",
    "    \n",
    "    tf[domain] = {i: c[i]/n_words_in_domain[domain] for i in c}\n",
    "    \n",
    "    for w in tf[domain].keys():\n",
    "        df[w] = df[w] + 1 if w in df.keys() else 1\n",
    "    \n",
    "# df = Counter(all_docs) # frequency of words across entire document\n",
    "idf = {w: np.log(2/(df[w])) for w in df} # 2 is number of domains\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for domain in domains:\n",
    "    tf_idf[domain] = {t : tf[domain][t] * idf[t] for t in tf[domain]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed1715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ad7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(top_words_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270bbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in domains:\n",
    "    plt.figure()\n",
    "    x, y = zip(*top_words_domain[domain][0:20])\n",
    "    plt.bar(x=x,height=y)\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599edc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in domains:\n",
    "    plt.figure()\n",
    "    x, y = zip(*top_words_domain[domain][0:20])\n",
    "    plt.bar(x=x,height=y)\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d468553",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in domains:\n",
    "    p = plt.figure(figsize=(6,6))\n",
    "#     x,y = zip(*tf_idf[domain].items())\n",
    "    df = pd.DataFrame.from_dict(tf_idf[domain],orient='index').rename(columns={0:'tf-idf'}).sort_values('tf-idf', ascending=False).iloc[0:15]\n",
    "    plt.barh(y = df.index, width=df['tf-idf'])\n",
    "    p.get_axes()[0].invert_yaxis()\n",
    "    plt.ylabel('tf-idf')\n",
    "    plt.subplots_adjust(left=0.2, bottom=0.4)\n",
    "    plt.savefig('td-idf-{}-top-15.pdf'.format(domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5892601",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(tf_idf['structures'],orient='index').rename(columns={0:'tf-idf'}).sort_values('tf-idf', ascending=False).iloc[0:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce871b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(tf_idf['drawing'],orient='index').rename(columns={0:'tf-idf'}).sort_values('tf-idf', ascending=False).iloc[0:30]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092d498",
   "metadata": {},
   "source": [
    "#### By subdomain (where entire corpus is the collection of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd017845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by subdomain\n",
    "all_words = {}\n",
    "top_words = {}\n",
    "n_words_in_subdomain = {}\n",
    "tf = {}\n",
    "df = {}\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for subdomain in (subdomains['structures'] + subdomains['drawing']):\n",
    "    \n",
    "    doc = [d for sublist in df_trial[df_trial.subdomain==subdomain]['lemmatized_filtered_whats'] \n",
    "                                              for item in sublist\n",
    "                                              for d in item]\n",
    "    \n",
    "    all_docs += doc\n",
    "    \n",
    "    c = Counter(doc)\n",
    "    all_words[subdomain] = c\n",
    "    top_words[subdomain] = c.most_common(30)\n",
    "    \n",
    "    n_words_in_subdomain[subdomain] = len(c)\n",
    "    \n",
    "    tf[subdomain] = {i: c[i]/n_words_in_subdomain[subdomain] for i in c}\n",
    "    \n",
    "    for w in tf[subdomain].keys():\n",
    "        df[w] = df[w] + 1 if w in df.keys() else 1\n",
    "    \n",
    "idf = {t: np.log(8/(df[t])) for t in df} # 2 is number of subdomains\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for subdomain in (subdomains['structures'] + subdomains['drawing']):\n",
    "    tf_idf[subdomain] = {t : tf[subdomain][t] * idf[t] for t in tf[subdomain]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdomain in (subdomains['structures'] + subdomains['drawing']):\n",
    "    p = plt.figure(figsize=(6,6))\n",
    "#     x,y = zip(*tf_idf[domain].items())\n",
    "    df = pd.DataFrame.from_dict(tf_idf[subdomain],orient='index').rename(columns={0:'tf-idf'}).sort_values('tf-idf', ascending=False).iloc[0:15]\n",
    "    plt.barh(y = df.index, width=df['tf-idf'])\n",
    "    plt.xlim(0,4)\n",
    "    p.get_axes()[0].invert_yaxis()\n",
    "    plt.ylabel('tf-idf')\n",
    "    plt.title(subdomain)\n",
    "    plt.subplots_adjust(left=0.2, bottom=0.4)\n",
    "#     plt.savefig('td-idf-{}-top-15.pdf'.format(domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(tf_idf['castle'],orient='index').rename(columns={0:'tf-idf'}).sort_values('tf-idf', ascending=False).iloc[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117714ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(tf_idf['castle'],orient='index').rename(columns={0:'tf-idf'}).sort_values('tf-idf', ascending=False).iloc[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a451ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf28ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59585b6c",
   "metadata": {},
   "source": [
    "#### do references to 'block' or similar seem biased towards any particular subdomains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks = df_trial_whats[['gameID','trial_num','subdomain','domain','block','blocks','brick','bricks','red','blue']]\n",
    "df_blocks['block_sum'] = df_blocks[['block','blocks','brick','bricks','red','blue']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9582da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,8))\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "            data = df_blocks,\n",
    "            x = 'subdomain',\n",
    "            order = subdomains['drawing'] +  subdomains['structures'],\n",
    "            palette= {**domain_palettes['drawing'],**domain_palettes_light['structures']},\n",
    "            y = 'block_sum')\n",
    "_ = plt.xticks(rotation = 60)\n",
    "plt.savefig('./plots/what_word_sum_subdomains.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b841d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a48a56a",
   "metadata": {},
   "source": [
    "## Correlations across different DSLs (very exploratory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acc69a",
   "metadata": {},
   "source": [
    "### instruction length vs. program length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70709921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlating instruction length with n_strokes\n",
    "\n",
    "df_trial.groupby(['domain','subdomain'])[['what_word_mean','base_program_length']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ed2e8",
   "metadata": {},
   "source": [
    "### vocabulary size vs. library size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aab961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['dreamcoder_program_dsl_0_n_unique_tokens'] = \\\n",
    "    df_trial.dreamcoder_program_dsl_0_tokens.apply(lambda x: len(pd.unique(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44570d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlating library size with vocabulary size]\n",
    "\n",
    "df_trial.groupby('subdomain')[['n_unique_whats', 'dreamcoder_program_dsl_0_n_unique_tokens']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tokens(series):\n",
    "    return(list(np.unique([token for list_ in series for token in list_])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.groupby(['domain','subdomain']).apply(lambda x: unique_tokens(x['low_level_parts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.groupby(['domain','subdomain']).apply(lambda x: unique_tokens(x['mid_level_parts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3656830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.groupby(['domain','subdomain']).apply(lambda x: unique_tokens(x['high_level_parts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = df_trial.groupby('subdomain')[['what_word_mean',\n",
    "                               'n_blocks',\n",
    "                               'low_level_prog_length',\n",
    "                               'mid_level_prog_length',\n",
    "                               'high_level_prog_length']].corr() #'tower_level_prog_length'\n",
    "\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_steps = df_trial_topdownabs.groupby('subdomain')[['n_steps',\n",
    "                               'n_blocks',\n",
    "                               'low_level_prog_length',\n",
    "                               'mid_level_prog_length',\n",
    "                               'high_level_prog_length']].corr() #'tower_level_prog_length'\n",
    "\n",
    "corrs_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819dbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_steps = df_trial_topdownabs.groupby('subdomain')[['n_unique_whats',\n",
    "                               'n_blocks',\n",
    "                               'low_level_prog_unique_tokens',\n",
    "                               'mid_level_prog_unique_tokens',\n",
    "                               'high_level_prog_unique_tokens']].corr() #'tower_level_prog_length'\n",
    "\n",
    "corrs_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_col = corrs_steps['n_unique_whats'].reset_index()\n",
    "\n",
    "corrs_series = corrs_col[corrs_col.level_1 != 'n_unique_whats']\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks(rotation=90)\n",
    "p = sns.lineplot(data=corrs_series, \n",
    "             x='level_1', \n",
    "             y='n_unique_whats',\n",
    "             hue='subdomain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ae927",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b54c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial_topdownabs[['n_steps',\n",
    "                         'n_blocks',\n",
    "                         'low_level_prog_length',\n",
    "                         'mid_level_prog_length',\n",
    "                         'high_level_prog_length']].corr() #'tower_level_prog_length'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5585bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structures, n blocks vs mean word count for that stim\n",
    "\n",
    "# just grab means (i.e. only one row per item needed)\n",
    "df = df_trial_topdownabs[df_trial_topdownabs.subdomain == 'castle'].groupby(['domain','subdomain','stimId']).first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "s = sns.scatterplot(data = df,\n",
    "                y = 'what_word_mean',\n",
    "                x = 'low_level_prog_length',\n",
    "#                 hue='n_steps',\n",
    "                alpha=0.6)\n",
    "\n",
    "# s.plot([0,1],[0,1], \n",
    "#        transform=s.transAxes, \n",
    "#        color='.3',\n",
    "#        linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73465e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial_topdownabs[['n_unique_whats',\n",
    "                               'low_level_prog_unique_tokens',\n",
    "                               'mid_level_prog_unique_tokens',\n",
    "                               'high_level_prog_unique_tokens']].corr() #'tower_level_prog_unique_tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial_topdownabs_cities[['n_unique_whats',\n",
    "                               'low_level_prog_unique_tokens',\n",
    "                               'mid_level_prog_unique_tokens',\n",
    "                               'high_level_prog_unique_tokens']].corr() # 'tower_level_prog_unique_tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01938b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find library size for each tower, only including abstractions up to that level language\n",
    "\n",
    "df_trial.subdomain.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate with number of words used to describe the tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.stimURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fca6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct libraries with different kinds of abstraction\n",
    "\n",
    "# simple part abstractions (e.g. repeated motif)\n",
    "\n",
    "# + types of simple abstractions (e.g. repeated motif type A)\n",
    "\n",
    "# + transformations (e.g. mirror, repeat, stack)\n",
    "\n",
    "# + transformation parameters (e.g. repeat n times)\n",
    "\n",
    "# + mid-level abstractions (e.g. skyscraper wall)\n",
    "\n",
    "# + mid-level abstractions parameters (e.g. wide vs. narror skyscraper)\n",
    "\n",
    "# + high-level abstractions (e.g. entire skyscraper)\n",
    "\n",
    "# + high-level abstractions types (e.g. entire skyscraper id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
